



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-3.2.0">
    
    
      
        <title>Intelligent Systems - Notes</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/application.572ca0f0.css">
      
        <link rel="stylesheet" href="assets/stylesheets/application-palette.22915126.css">
      
      
        
        
        <meta name="theme-color" content="#009688">
      
    
    
      <script src="assets/javascripts/modernizr.8c900955.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="assets/fonts/material-icons.css">
    
    
      <link rel="stylesheet" href="stylesheets/extra.css">
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="teal" data-md-color-accent="teal">
  
    <svg class="md-svg">
      <defs>
        
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#intelligent-systems" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="index.html" title="Notes" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                Notes
              </span>
              <span class="md-header-nav__topic">
                Intelligent Systems
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="index.html" title="Notes" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    Notes
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="index.html" title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="BodyBuilding.html" title="Body Building and general nutrition" class="md-nav__link">
      Body Building and general nutrition
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="ComputerSystems2.html" title="Computer Systems II" class="md-nav__link">
      Computer Systems II
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="DistributedSystemsAndNetworks.html" title="Distributed Systems & Networks" class="md-nav__link">
      Distributed Systems & Networks
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Intelligent Systems
      </label>
    
    <a href="IntelligentSystems.html" title="Intelligent Systems" class="md-nav__link md-nav__link--active">
      Intelligent Systems
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#classical-ai" title="Classical AI" class="md-nav__link">
    Classical AI
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#blind-search" title="Blind Search" class="md-nav__link">
    Blind Search
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#problem-types" title="Problem Types" class="md-nav__link">
    Problem Types
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#single-state-problem-formulation" title="Single-state Problem Formulation" class="md-nav__link">
    Single-state Problem Formulation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#states-vs-nodes" title="States vs Nodes" class="md-nav__link">
    States vs Nodes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#search-strategies" title="Search Strategies" class="md-nav__link">
    Search Strategies
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#searching-using-tree-search" title="Searching using Tree Search" class="md-nav__link">
    Searching using Tree Search
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#repeated-states" title="Repeated States" class="md-nav__link">
    Repeated States
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bidirectional-search" title="Bidirectional Search" class="md-nav__link">
    Bidirectional Search
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#heuristic-search" title="Heuristic Search" class="md-nav__link">
    Heuristic Search
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#history-and-philosophy" title="History and Philosophy" class="md-nav__link">
    History and Philosophy
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#local-search" title="Local Search" class="md-nav__link">
    Local Search
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#constraint-satisfaction-problems" title="Constraint satisfaction problems" class="md-nav__link">
    Constraint satisfaction problems
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#csps-come-in-different-variants" title="CSPs come in different variants" class="md-nav__link">
    CSPs come in different variants
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#constraints-come-in-different-variants-too" title="Constraints come in different variants too" class="md-nav__link">
    Constraints come in different variants too
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adversarial-games-minimax" title="Adversarial Games: Minimax" class="md-nav__link">
    Adversarial Games: Minimax
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alpha-beta-pruning" title="Alpha-beta Pruning" class="md-nav__link">
    Alpha-beta Pruning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#planning" title="Planning" class="md-nav__link">
    Planning
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#partial-order-planning" title="Partial-order Planning" class="md-nav__link">
    Partial-order Planning
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#biologicallystatistically-inspired-ai" title="Biologically/Statistically Inspired AI" class="md-nav__link">
    Biologically/Statistically Inspired AI
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classification" title="Classification" class="md-nav__link">
    Classification
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reasoning" title="Reasoning" class="md-nav__link">
    Reasoning
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bayesian-belief-update" title="Bayesian Belief Update" class="md-nav__link">
    Bayesian Belief Update
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#complex-knowledge-representation" title="Complex Knowledge Representation" class="md-nav__link">
    Complex Knowledge Representation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descision-making" title="Descision Making" class="md-nav__link">
    Descision Making
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bandit-theory" title="Bandit Theory" class="md-nav__link">
    Bandit Theory
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-armed-bandit-mab-model" title="Multi-armed Bandit (MAB) Model" class="md-nav__link">
    Multi-armed Bandit (MAB) Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#other-algorithms" title="Other Algorithms" class="md-nav__link">
    Other Algorithms
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-learning" title="Reinforcement Learning" class="md-nav__link">
    Reinforcement Learning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#states-actions-and-rewards" title="States, Actions and Rewards" class="md-nav__link">
    States, Actions and Rewards
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#markov-decision-processes-mdp-and-markov-chain" title="Markov Decision Processes (MDP) and Markov Chain" class="md-nav__link">
    Markov Decision Processes (MDP) and Markov Chain
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="ProgrammingLanguageConcepts.html" title="Programming Language Concepts" class="md-nav__link">
      Programming Language Concepts
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#classical-ai" title="Classical AI" class="md-nav__link">
    Classical AI
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#blind-search" title="Blind Search" class="md-nav__link">
    Blind Search
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#problem-types" title="Problem Types" class="md-nav__link">
    Problem Types
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#single-state-problem-formulation" title="Single-state Problem Formulation" class="md-nav__link">
    Single-state Problem Formulation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#states-vs-nodes" title="States vs Nodes" class="md-nav__link">
    States vs Nodes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#search-strategies" title="Search Strategies" class="md-nav__link">
    Search Strategies
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#searching-using-tree-search" title="Searching using Tree Search" class="md-nav__link">
    Searching using Tree Search
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#repeated-states" title="Repeated States" class="md-nav__link">
    Repeated States
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bidirectional-search" title="Bidirectional Search" class="md-nav__link">
    Bidirectional Search
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#heuristic-search" title="Heuristic Search" class="md-nav__link">
    Heuristic Search
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#history-and-philosophy" title="History and Philosophy" class="md-nav__link">
    History and Philosophy
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#local-search" title="Local Search" class="md-nav__link">
    Local Search
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#constraint-satisfaction-problems" title="Constraint satisfaction problems" class="md-nav__link">
    Constraint satisfaction problems
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#csps-come-in-different-variants" title="CSPs come in different variants" class="md-nav__link">
    CSPs come in different variants
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#constraints-come-in-different-variants-too" title="Constraints come in different variants too" class="md-nav__link">
    Constraints come in different variants too
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adversarial-games-minimax" title="Adversarial Games: Minimax" class="md-nav__link">
    Adversarial Games: Minimax
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alpha-beta-pruning" title="Alpha-beta Pruning" class="md-nav__link">
    Alpha-beta Pruning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#planning" title="Planning" class="md-nav__link">
    Planning
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#partial-order-planning" title="Partial-order Planning" class="md-nav__link">
    Partial-order Planning
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#biologicallystatistically-inspired-ai" title="Biologically/Statistically Inspired AI" class="md-nav__link">
    Biologically/Statistically Inspired AI
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classification" title="Classification" class="md-nav__link">
    Classification
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reasoning" title="Reasoning" class="md-nav__link">
    Reasoning
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bayesian-belief-update" title="Bayesian Belief Update" class="md-nav__link">
    Bayesian Belief Update
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#complex-knowledge-representation" title="Complex Knowledge Representation" class="md-nav__link">
    Complex Knowledge Representation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descision-making" title="Descision Making" class="md-nav__link">
    Descision Making
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bandit-theory" title="Bandit Theory" class="md-nav__link">
    Bandit Theory
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-armed-bandit-mab-model" title="Multi-armed Bandit (MAB) Model" class="md-nav__link">
    Multi-armed Bandit (MAB) Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#other-algorithms" title="Other Algorithms" class="md-nav__link">
    Other Algorithms
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-learning" title="Reinforcement Learning" class="md-nav__link">
    Reinforcement Learning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#states-actions-and-rewards" title="States, Actions and Rewards" class="md-nav__link">
    States, Actions and Rewards
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#markov-decision-processes-mdp-and-markov-chain" title="Markov Decision Processes (MDP) and Markov Chain" class="md-nav__link">
    Markov Decision Processes (MDP) and Markov Chain
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="intelligent-systems">Intelligent Systems<a class="headerlink" href="#intelligent-systems" title="Permanent link">⇦</a></h1>
<details class="bug"><summary>Not widely sourced</summary><p>Content heavily lifted from <strong>Matthew Barnes</strong> (Southampton Uni CompSci student)! </p>
<p><strong>All knowledge presented here is sourced from his public notes.</strong></p>
</details>
<details class="success"><summary>University course</summary><p>These notes were made for the University of Southampton COMP2208 module.</p>
</details>
<h2 id="classical-ai">Classical AI<a class="headerlink" href="#classical-ai" title="Permanent link">⇦</a></h2>
<details class="info"><summary>Blind Search</summary><h3 id="blind-search">Blind Search<a class="headerlink" href="#blind-search" title="Permanent link">⇦</a></h3>
<p>An uninformed search generally used when knowledge is restricted.</p>
<h4 id="problem-types">Problem Types<a class="headerlink" href="#problem-types" title="Permanent link">⇦</a></h4>
<details class="warning"><summary>Single-state problem</summary><ul>
<li>Deterministic</li>
<li>Observable</li>
<li>State is always known</li>
</ul>
<div class="admonition example">
<p>The shortest path problem is an example of this.</p>
</div>
</details>
<details class="warning"><summary>Sensorless / multi-state problem</summary><ul>
<li>Deterministic</li>
<li>Non-Observable</li>
<li>Initial state could be anything</li>
</ul>
<div class="admonition example">
<p>Guessing a random number. The random number could be anything but it will always be found.</p>
</div>
</details>
<details class="warning"><summary>Contingency problem</summary><ul>
<li>Non-Deterministic</li>
<li>Partially observable</li>
</ul>
<div class="admonition info">
<p class="admonition-title"> </p>
<p>You have to perform an action and observe the reactions to move towards a solution.</p>
</div>
</details>
<details class="warning"><summary>Exploration problem</summary><ul>
<li>Unknown state space</li>
<li>Don't even know what effect actions have</li>
<li>"explore" the environment to solve</li>
</ul>
<div class="admonition example">
<p>Robot exploration as the robot has no idea where it is, it just reacts to the environment. </p>
</div>
</details>
<h4 id="single-state-problem-formulation">Single-state Problem Formulation<a class="headerlink" href="#single-state-problem-formulation" title="Permanent link">⇦</a></h4>
<table>
<thead>
<tr>
<th>Step</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>Initial state</td>
<td>The state we start in</td>
<td>When you start a chess game</td>
</tr>
<tr>
<td>Action or successor function</td>
<td>The transition from one state to another</td>
<td>When you move a piece in a chess game</td>
</tr>
<tr>
<td>Goal test</td>
<td>Checks if solution has been found</td>
<td>Checkmate is reached</td>
</tr>
<tr>
<td>Path cost</td>
<td>The cost of getting to the solution</td>
<td>Moves taken</td>
</tr>
</tbody>
</table>
<div class="admonition tip">
<p class="admonition-title"> </p>
<p>There are multiple path costs that a chess algorithm can adopt. </p>
<p>One could be ‘number of opponent pieces’. This would make the algorithm <strong>aggressive</strong>, since it would try to reduce the number of opponent pieces as much as it can. </p>
<p>Another could be ‘number of moves taken’. This would make the algorithm more <strong>strategic</strong>, achieving a checkmate in the least amount of moves.</p>
</div>
<h4 id="states-vs-nodes">States vs Nodes<a class="headerlink" href="#states-vs-nodes" title="Permanent link">⇦</a></h4>
<ul>
<li>A state is <strong>not</strong> a node</li>
<li>A state is a representation of a physical configuration</li>
<li>A node is a data structure consituting part of a search tree used to find the solution</li>
<li>A state is a property of a node</li>
</ul>
<h4 id="search-strategies">Search Strategies<a class="headerlink" href="#search-strategies" title="Permanent link">⇦</a></h4>
<details class="warning"><summary>Strategies are evaluated with the following factors</summary><ul>
<li><strong>Completeness</strong> does it always find a solution if it exists?</li>
<li><strong>Time Complexity</strong> how does time taken grow as the solution depth increases</li>
<li><strong>Space complexity</strong> how does the number of nodes in memory grow</li>
<li><strong>Optimality</strong> does it always find a least-cost solution?</li>
</ul>
</details>
<details class="warning"><summary>Time and space complexity are measured in terms of</summary><ul>
<li><strong>b</strong> how many children each node reaches (maximum branching factor)</li>
<li><strong>d</strong> depth of least-cost optimal solution</li>
<li><strong>m</strong> maximum depth of the state space (may be infinite)</li>
</ul>
</details>
<h4 id="searching-using-tree-search">Searching using Tree Search<a class="headerlink" href="#searching-using-tree-search" title="Permanent link">⇦</a></h4>
<ul>
<li>To find the solution to a problem you need to find the path from the initial state to a goal state</li>
<li>This is most commonly achieved using a type of tree search</li>
<li>There are four kinds of tree search algorithms used for this</li>
</ul>
<table>
<thead>
<tr>
<th>Tree search</th>
<th>Complexity</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Breadth-first search</td>
<td><strong>Complete</strong> yes if branching factor is finite <br><br> <strong>Time</strong> O(b<sup>d+1</sup>) <br><br> <strong>Space</strong> O(b<sup>d+1</sup>) <br><br> <strong>Optimal</strong> Yes if step cost is constant</td>
<td>In breadth-first search, all of the nodes at the current depth level are searched before moving onto the next depth level. <br><br> This is <strong>complete</strong> (assuming the branching factor isn’t infinite), <strong>even with infinite depth or loops</strong>. <br><br> Only problem is, the <strong>space complexity is terrible</strong>. However, breadth-first search is <strong>optimal</strong> (finds the least-cost solution) <strong>if the step cost is constant</strong>. This is true because it finds the shallowest goal node.</td>
</tr>
<tr>
<td>Depth-first search</td>
<td><strong>Complete</strong> No, if depth is infinite (or has loops) it'll go on infinitely <br><br> <strong>Time</strong> O(b<sup>m</sup>) <br><br> <strong>Space</strong> O(bm) <br><br> <strong>Optimal</strong> No, because deeper solution may be found first</td>
<td>In depth-first search, it starts at the root and goes all the way down to the far-left leaf, then backtracks and expands shallower nodes. <br><br> The <strong>space complexity is great (it’s linear)</strong> because <strong>branches can be released from memory if no solution was found in them</strong>. However, it is <strong>not complete</strong></td>
</tr>
<tr>
<td>Depth-limited search</td>
<td><strong>Complete</strong> No, because the solution may be deeper than the limit <br><br> <strong>Time</strong> O(b<sup>n</sup>) <br><br> <strong>Space</strong> O(bn) <br><br> <strong>Optimal</strong> No, because deeper solution might be found first</td>
<td>It’s the <strong>same as depth-first search</strong>, but there’s a <strong>depth limit</strong> (<strong>n</strong>), and <strong>anything below that depth limit doesn’t exist</strong></td>
</tr>
<tr>
<td>Iterative deepening search</td>
<td><strong>Complete</strong> Yes <br><br> <strong>Time</strong> O(b<sup>d</sup>) <br><br> <strong>Space</strong> O(bd) <br><br> <strong>Optimal</strong> Yes, if step cost is constant</td>
<td>This is an applied version of depth-limited search, where we <strong>start from depth limit 0 and increment the depth limit until we find our goal</strong>. <br><br> We do this to combine <strong>depth-first search’s space efficiency</strong> with <strong>breadth-first search’s completeness</strong>. Although we get those benefits, <strong>IDS is a little bit slower than depth-first</strong>.</td>
</tr>
</tbody>
</table>
<h4 id="repeated-states">Repeated States<a class="headerlink" href="#repeated-states" title="Permanent link">⇦</a></h4>
<ul>
<li>When searching we may come across an already visited state</li>
<li>In a deterministic system this will always move to the same next state</li>
<li>Because of this we can <strong>recognise already visited states and prevent further exploration</strong></li>
<li>This <strong>converts our tree searches into graph searches</strong> (as we are no longer searching a tree)</li>
</ul>
<h4 id="bidirectional-search">Bidirectional Search<a class="headerlink" href="#bidirectional-search" title="Permanent link">⇦</a></h4>
<ul>
<li>A bidirectional search does two searches</li>
<li>One starts at the initial state and the other from the goal state</li>
<li>This is quicker as <strong>b<sup>d/2</sup> + b<sup>d/2</sup> is much less than b<sup>d</sup></strong></li>
<li>At each iteration, the node is checked if it has been <strong>discovered by both searches</strong></li>
<li>If it has a solution has been found</li>
<li>This is <strong>only efficient if the predecessor of a node can be easily computed</strong></li>
</ul>
</details>
<details class="info"><summary>Heuristic Search</summary><h3 id="heuristic-search">Heuristic Search<a class="headerlink" href="#heuristic-search" title="Permanent link">⇦</a></h3>
</details>
<details class="info"><summary>History and Philosophy</summary><h3 id="history-and-philosophy">History and Philosophy<a class="headerlink" href="#history-and-philosophy" title="Permanent link">⇦</a></h3>
</details>
<details class="info"><summary>Local Search</summary><h3 id="local-search">Local Search<a class="headerlink" href="#local-search" title="Permanent link">⇦</a></h3>
</details>
<details class="info"><summary>Constraint satisfaction problems</summary><h3 id="constraint-satisfaction-problems">Constraint satisfaction problems<a class="headerlink" href="#constraint-satisfaction-problems" title="Permanent link">⇦</a></h3>
<ul>
<li>In a standard search problem the state is viewed as a black box</li>
<li>A state can be any data structure that supports<ul>
<li>A successor function</li>
<li>An objective function (fitness)</li>
<li>A goal test</li>
</ul>
</li>
<li>In constraint satisfaction problem we will <strong>define the data structure of the state</strong></li>
<li>In particular, a state is defined by variables <span><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span> with values from the domain <span><span class="MathJax_Preview">D_{i}</span><script type="math/tex">D_{i}</script></span></li>
<li>In the goal test we use a set of constraints</li>
</ul>
<details class="example"><summary>Graph colouring problem</summary><p>We try to colour regions with three different colours such that adjacent regions have a different colour.
<img alt="Map1" src="assets/IntelligentSystems/Map1.png" /></p>
<p>For this map the problem is defined as follows</p>
<ul>
<li>Variables are the regions WA, NT, Q, NSW, V, SA, T       </li>
<li>Domains are the three colours <span><span class="MathJax_Preview">D_{i} = \{red, green, blue\}</span><script type="math/tex">D_{i} = \{red, green, blue\}</script></span></li>
</ul>
<p>For example WA and NT cannot have the same colour</p>
<ul>
<li><strong>Complete</strong> Every variable is assigned a value</li>
<li><strong>Consistent</strong> No constraint violated</li>
</ul>
<p>This is an example solution for this problem</p>
<p><img alt="Map1Solution" src="assets/IntelligentSystems/Map1Solution.png" /></p>
<ul>
<li>Every region has a colour with no two adjacent regions sharing a colour</li>
<li>This solution is therefore <strong>complete</strong> and <strong>consistent</strong></li>
<li>This can be generalised as a <strong>constraint graph</strong></li>
<li>The nodes are the variables and the edges are the constraint</li>
<li>No two connected nodes can have the same assignment</li>
<li>In addition, the resulting graph is a binary CSP because each constraint relates two variables (each edge connects two nodes)</li>
</ul>
<p><img alt="constraintgraph" src="assets/IntelligentSystems/constraintgraph.png" /></p>
</details>
<h4 id="csps-come-in-different-variants">CSPs come in different variants<a class="headerlink" href="#csps-come-in-different-variants" title="Permanent link">⇦</a></h4>
<details class="warning"><summary>If the CSP has <strong>discrete variables</strong></summary><p>And the <strong>domain is finite</strong></p>
<ul>
<li>We have <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> variables and the domain size is <span><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span></li>
<li>Therefore there exists <span><span class="MathJax_Preview">O(d^{n})</span><script type="math/tex">O(d^{n})</script></span> complete assignments</li>
<li>Examples are boolean CSPs, including boolean satisfiability (SAT) which is NP-Complete</li>
</ul>
<p>And the <strong>domain is infinite</strong></p>
<ul>
<li>Examples of infinite domains are integers and strings</li>
<li>An example for such a CSP is job scheduling where the variables are the start/end days for each job</li>
<li>The constraints could be defined like <span><span class="MathJax_Preview">StartJob_{1} + 5 &lt; StartJob_{3}</span><script type="math/tex">StartJob_{1} + 5 < StartJob_{3}</script></span></li>
</ul>
</details>
<details class="warning"><summary>If the CSP has <strong>continuous variables</strong></summary><ul>
<li>An example is start/end times of the Hubble Space Telescope observations</li>
<li>The constraints are linear constraints solvable in polynomial time by linear programming</li>
</ul>
</details>
<h4 id="constraints-come-in-different-variants-too">Constraints come in different variants too<a class="headerlink" href="#constraints-come-in-different-variants-too" title="Permanent link">⇦</a></h4>
<ul>
<li><strong>Unary</strong> constraints involve a single variable, for example SA <span><span class="MathJax_Preview">\neq</span><script type="math/tex">\neq</script></span> green</li>
<li><strong>Binary</strong> constaints involve pairs of variables, for example SA <span><span class="MathJax_Preview">\neq</span><script type="math/tex">\neq</script></span> WA</li>
<li><strong>Higher-order</strong> constraints involve 3 or more variables, for example <span><span class="MathJax_Preview">X_{1} + X_{2} = X_{3}</span><script type="math/tex">X_{1} + X_{2} = X_{3}</script></span></li>
</ul>
<details class="info"><summary>Some more real-world CSPs are</summary><ul>
<li>Assignment problems, e.g who teaches what class</li>
<li>Timetabling problems, e.g which class is offered when and where</li>
<li>Transportation scheduling</li>
<li>Factory scheduling</li>
</ul>
<p><strong>Note how many real-world problems involve real-valued (continuous) variables!</strong></p>
</details>
<div class="admonition bug">
<p class="admonition-title">Missing developing an algorithm for CSPs</p>
</div>
</details>
<details class="info"><summary>Adversarial Games: Minimax</summary><h3 id="adversarial-games-minimax">Adversarial Games: Minimax<a class="headerlink" href="#adversarial-games-minimax" title="Permanent link">⇦</a></h3>
</details>
<details class="info"><summary>Alpha-beta Pruning</summary><h3 id="alpha-beta-pruning">Alpha-beta Pruning<a class="headerlink" href="#alpha-beta-pruning" title="Permanent link">⇦</a></h3>
<ul>
<li>Alpha-beta pruning goes even beyond minimax</li>
<li>We have strong and weak methods to improve minimax</li>
<li><strong>Strong</strong> methods take into <strong>account the game itself</strong>, ie board symmetry in tic tac toe</li>
<li><strong>Weak</strong> methods can apply to <strong>any game</strong>, one of which is alpha-beta pruning</li>
</ul>
<details class="question"><summary>What is pruning?</summary><ul>
<li>A good example of pruning is the symmetry in tic tac toe</li>
<li>A part of the full game tree looks like this</li>
</ul>
<p><img alt="pruning1" src="assets/IntelligentSystems/pruning1.png" /></p>
<ul>
<li>Some of these states are redudant, we can achieve them just by rotating the board</li>
</ul>
<p><img alt="pruning2" src="assets/IntelligentSystems/pruning2.png" /></p>
<ul>
<li>By removing the redundant states we can prune the search tree a lot</li>
<li><strong>Not every game can be pruned in this way!</strong></li>
</ul>
</details>
<details class="warning"><summary>How does alpha-beta pruning work?</summary><p>Minimax explores the entire tree to a given ply depth</p>
<ul>
<li>It evaluates the leaves</li>
<li>It propagates the values of these leaves back up the tree</li>
</ul>
<p>Alpha-beta pruning performs DFS but allows us to prune/disregard certain branches of the tree</p>
<ul>
<li><strong>Alpha</strong> represents the <strong>lower bound</strong> on the node value. <strong>It is the worst we can do</strong>. It is associated with MAX nodes and since it is the worst it never decreases.</li>
<li><strong>Beta</strong> represents the <strong>upper bound</strong> on the node value. <strong>It is the best we can do</strong>. It is associated with MIN nodes and since it is the best it never increases.</li>
</ul>
<p>If the best we can do on the current branch is less than or equal to the worst we can do elsewhere, there is no point continuing this branch!</p>
<p>This is <strong>very similar to minimax</strong> except we can <strong>return early</strong> if the alpha/beta values indicate to do so</p>
</details>
<p>Alpha-beta is guarenteed to give the same values as minimax except we reduce the search space.</p>
<ul>
<li>If the tree is <strong>ordered</strong>, the <strong>time complexity is O(b<sup>d/2</sup>)</strong> (minimax is O(b<sup>d</sup>)!)</li>
<li>Perfect ordering <strong>is not possible</strong> as we wouldn't need alpha-beta in the first place!</li>
</ul>
</details>
<details class="info"><summary>Planning </summary><h3 id="planning">Planning<a class="headerlink" href="#planning" title="Permanent link">⇦</a></h3>
<p>A plan is a sequence of actions to perform tasks and achieve objectives</p>
<ul>
<li>This means generating and searching over possible plans</li>
</ul>
<p>The classical planning environment is fully oberservable, deterministic, finite, static and discrete. It helps to assist humans in practical applications such as</p>
<ul>
<li>Design and manufacuring</li>
<li>Military operations</li>
<li>Games</li>
<li>Space exploration</li>
</ul>
<div class="admonition warning">
<p class="admonition-title"> </p>
<p>In the real world we have a huge planning environment. We need need to reduce this environment by using heuristics and decomposing the problem.</p>
</div>
<details class="question"><summary>What is a planning language?</summary><p>In order to define plans we need a planning language</p>
<p>A good planning language should</p>
<ul>
<li>Be expressive enough to <strong>describe a wide variety of problems</strong></li>
<li>Be restrictive enough to <strong>allow efficient algorithms</strong> to operate on it</li>
<li>Be able to <strong>take advantage</strong> of the <strong>logical structure</strong> of the problem</li>
</ul>
<p>Examples of planning languages are</p>
<ul>
<li>The STRIPS (Stanford Research Institute Problem Solver) model</li>
<li>ADL (Action Description Language)</li>
</ul>
<p>Typically planning languages involve</p>
<ul>
<li><strong>States</strong> decompose the world into logical conditions and represent a state as a collection of these conditions, for example <code>OnTable(Box1) ⋀ OnFloor(Box2)</code></li>
<li><strong>Goals</strong> a goal is a partially specified state, if any state contains all the literals (conditions) of the goal then this state is considered a goal state</li>
<li><strong>Actions</strong> a precondition and an effect, we can only perform the effect if the precondition is already met, for example we can only put a box on a table if it isn't already on the table</li>
</ul>
<div class="admonition warning">
<p class="admonition-title"> </p>
<p>In an action there needs to exist a substitution for all variables in the precondition!</p>
</div>
</details>
<details class="example"><summary>Air cargo transport</summary><table>
<thead>
<tr>
<th>Part</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td>Initial State</td>
<td><code>Init( At(C1, SFO) ⋀ At(C2, JFK) ⋀ At(P1, SFO) ⋀ At(P2, JFK) ⋀ Cargo(C1) ⋀ Cargo(C2) ⋀ Plane(P1) ⋀ Plane(P2) ⋀ Airport(JFK) ⋀ Airport(SFO) )</code></td>
</tr>
<tr>
<td>Goal State</td>
<td><code>Goal( At(C1, JFK) ⋀ At(C2, SFO) )</code></td>
</tr>
<tr>
<td>Actions</td>
<td><code>Action( Load(c, p, a)</code><br> <code>PRECOND : At(c, a) ⋀ At(p, a) ⋀ Cargo(c) ⋀ Plane(p) ⋀ Airport(a)</code><br><code>EFFECT : ¬At(c, a) ⋀ In(c, p) )</code></td>
</tr>
<tr>
<td>Example plan</td>
<td><code>[Load(C1, P1, SFO), Fly(P1, SFO, JFK),</code><br><code>Load(C2, P2, JFK), Fly(P2, JFK, SFO)]</code></td>
</tr>
</tbody>
</table>
</details>
<details class="example"><summary>Spare tire problem</summary><table>
<thead>
<tr>
<th>Part</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td>Initial State</td>
<td><code>Init(At(Flat, Axle) ⋀ At(Spare, trunk))</code></td>
</tr>
<tr>
<td>Goal State</td>
<td><code>Goal(At(Spare, Axle))</code></td>
</tr>
<tr>
<td>Actions</td>
<td><code>Action( Remove(Spare, Trunk)</code><br> <code>PRECOND : At(Spare, Trunk)</code><br><code>EFFECT : ¬At(Spare, Trunk) ⋀ At(Spare, Ground) )</code></td>
</tr>
<tr>
<td></td>
<td><code>Action( Remove(Flat, Axle)</code><br><code>PRECOND : At(Flat, Axle)</code><br><code>EFFECT : ¬At(Flat, Axle) ⋀ At(Flat, Ground) )</code></td>
</tr>
<tr>
<td></td>
<td><code>Action(PutOn(Spare, Axle)</code><br><code>PRECOND : At(Spare, Ground) ⋀ ¬At(Flat, Axle)</code><br><code>`EFFECT : At(Spare, Axle) ⋀ ¬At(Spare, Ground) )</code></td>
</tr>
<tr>
<td></td>
<td><code>Action( LeaveOvernight</code><br><code>EFFECT : ¬At(Spare, Ground) ⋀ ¬At(Spare, Axle) ⋀ ¬At(Spare, trunk) ⋀ ¬At(Flat, Ground) ⋀ ¬At(Flat, Axle) )</code></td>
</tr>
<tr>
<td>Example plan</td>
<td><code>[Remove(Flat, Axle),</code><br><code>Remove(Spare, Trunk),</code><br><code>PutOn(Spare, Axle)]</code></td>
</tr>
</tbody>
</table>
</details>
<p>There are two main approaches to making an algorithm which can come up with a plan</p>
<ul>
<li><strong>Forward</strong> search, <strong>progression</strong> planners do forward state-space search. They consider the effect of all possible actions in a given state and <strong>search from the initial state to the goal state</strong>.</li>
<li><strong>Backward</strong> search, <strong>regression</strong> planners do backward state-space search. They consider what must have been true in the previous state to a goal state and therefore <strong>search from the goal state to the initial state</strong>.</li>
</ul>
<p>Forward searches are nothing more than any complete graph search algorithm, for example A*.</p>
<p>Backward searches find actions which have effects that are currently satisfied and pre-conditions which are preferred. <strong>These actions must not undo desired literals</strong>. This prevents unnecessary actions being considered and therefore reduces the branching factor.</p>
<details class="warning"><summary>Both progression and regression perform poorly without a good heurisic</summary><p>To find an admissile heuristic there are generally two approaches</p>
<ul>
<li>Use the optimal solution to the relaxed problem, e.g. where we remove all preconditions from actions</li>
<li>Use the sub-goal independence assumption: The cost of solving a conjunction of subgoals is approximated by the sum of the costs of solving the sub-problems independently.</li>
</ul>
</details>
<h4 id="partial-order-planning">Partial-order Planning<a class="headerlink" href="#partial-order-planning" title="Permanent link">⇦</a></h4>
<p>Progression and regression planning are forms of <strong>totally ordered</strong> plan searches</p>
<ul>
<li>We can improve this by specifying some actions which can be done in parallel</li>
<li>For these actions it doesn't matter which comes first</li>
</ul>
<details class="warning"><summary>Each plan has 4 components</summary><ul>
<li>A set of actions (steps of the plan)</li>
<li>A set of ordering constraints <span><span class="MathJax_Preview">A &lt; B</span><script type="math/tex">A < B</script></span> (A has to come before B)</li>
<li>A set of causal links <span><span class="MathJax_Preview">A\xrightarrow{p}B</span><script type="math/tex">A\xrightarrow{p}B</script></span> or <span><span class="MathJax_Preview">A\xrightarrow{} p\xrightarrow{} B</span><script type="math/tex">A\xrightarrow{} p\xrightarrow{} B</script></span> (A achieves p for B)</li>
<li>A set of open preconditions</li>
</ul>
</details>
<details class="example"><summary>Shoe example</summary><table>
<thead>
<tr>
<th>Part</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td>Initial State</td>
<td><code>Init( )</code></td>
</tr>
<tr>
<td>Goal State</td>
<td><code>Goal( RightShoeOn ⋀ LeftShoeOn )</code></td>
</tr>
<tr>
<td>Actions</td>
<td><code>Action( RightShoe</code><br><code>PRECOND : RightSockOn</code><br><code>EFFECT : RightShoeOn )</code></td>
</tr>
<tr>
<td></td>
<td><code>Action( RightSock</code><br><code>PRECOND :</code><br><code>EFFECT : RightSockOn )</code></td>
</tr>
<tr>
<td></td>
<td><code>Action( LeftShoe</code><br><code>PRECOND : LeftSockOn</code><br><code>EFFECT : LeftShoeOn )</code></td>
</tr>
<tr>
<td></td>
<td><code>Action( LeftSock</code><br><code>PRECOND :</code><br><code>EFFECT : LeftSockOn )</code></td>
</tr>
</tbody>
</table>
<p><span><span class="MathJax_Preview">Actions = {Rightsock\\
             Rightshoe\\
             Leftsock\\
              Leftshoe\\
               Start\\
                Finish}</span><script type="math/tex">Actions = {Rightsock\\
             Rightshoe\\
             Leftsock\\
              Leftshoe\\
               Start\\
                Finish}</script></span></p>
<p><span><span class="MathJax_Preview">Orderings = {Rightsock &lt; Rightshoe \\
                Leftsock &lt; Leftshoe}</span><script type="math/tex">Orderings = {Rightsock < Rightshoe \\
                Leftsock < Leftshoe}</script></span></p>
<p><span><span class="MathJax_Preview">Links = {Rightsock \xrightarrow{Rightsockon} Rightshoe \\
            Leftsock \xrightarrow{Leftsockon} Leftshoe \\
            Rightshoe \xrightarrow{Rightshoeon} Finish \\
            Leftshoe \xrightarrow{Leftshowon} Finish}</span><script type="math/tex">Links = {Rightsock \xrightarrow{Rightsockon} Rightshoe \\
            Leftsock \xrightarrow{Leftsockon} Leftshoe \\
            Rightshoe \xrightarrow{Rightshoeon} Finish \\
            Leftshoe \xrightarrow{Leftshowon} Finish}</script></span></p>
<p>We can put either shoe on first it doesn't matter. This lets us create a <strong>partially-ordered plan</strong></p>
<p><img alt="partiallyorderedgraph" src="assets/IntelligentSystems/partiallyorderedgraph.png" /></p>
</details>
<div class="admonition question">
<p class="admonition-title"> </p>
<p><strong>When is a plan final?</strong></p>
<p>A consistent plan with no open preconditions is a solution. A plan is consistent if and only if there are no cycles in the ordering constraints and no conflicts with the links.</p>
<p>A POP is executed by repeatedly choosing any of the possible next actions.</p>
</div>
<div class="admonition bug">
<p class="admonition-title">Missing resolving POP</p>
</div>
</details>
<h2 id="biologicallystatistically-inspired-ai">Biologically/Statistically Inspired AI<a class="headerlink" href="#biologicallystatistically-inspired-ai" title="Permanent link">⇦</a></h2>
<details class="info"><summary>Classification</summary><h3 id="classification">Classification<a class="headerlink" href="#classification" title="Permanent link">⇦</a></h3>
</details>
<details class="info"><summary>Reasoning</summary><h3 id="reasoning">Reasoning<a class="headerlink" href="#reasoning" title="Permanent link">⇦</a></h3>
<p>Reasoning deals with updating our "belief model"</p>
<p>There are different approaches to reasoning</p>
<ul>
<li><strong>Logic/Rule based</strong> we build up basic rules (axioms) using logic which we can derive other rules from</li>
<li><strong>Stochastic reasoning</strong> reasoning based on probabilities</li>
</ul>
<details class="warning"><summary>Total Probability</summary><p>The total probabilty of an event B is defined as</p>
<div>
<div class="MathJax_Preview">P(B) = \sum_{a} P(B\, and\, (A=a)) = \sum_{a} P(B|A = a)P(A=a)</div>
<script type="math/tex; mode=display">P(B) = \sum_{a} P(B\, and\, (A=a)) = \sum_{a} P(B|A = a)P(A=a)</script>
</div>
<p>The total probability for event B is the sum of the probabilities for event B and all cases of event A</p>
</details>
<details class="warning"><summary>Conditional Probability</summary><p>The conditional probability of an event B is defined as</p>
<div>
<div class="MathJax_Preview">P(B|A) = \frac{P(A\, and\, B)}{P(A)}</div>
<script type="math/tex; mode=display">P(B|A) = \frac{P(A\, and\, B)}{P(A)}</script>
</div>
<div>
<div class="MathJax_Preview">P(A|B)P(B) = P(A\, and\, B) = P(B|A)P(A)</div>
<script type="math/tex; mode=display">P(A|B)P(B) = P(A\, and\, B) = P(B|A)P(A)</script>
</div>
<p><span><span class="MathJax_Preview">P(A|B)</span><script type="math/tex">P(A|B)</script></span> means "the probability of event A given that B is true"</p>
<details class="question"><summary>Why is it defined like this?</summary><p>Let's look at an example to explain this. We have a bag with red and blue marbles, and we want to know the probabilities of drawing a red/blue marble. We can visualise the probabilities for 2 consecutive draws like this</p>
<p><img alt="conditionalprob1" src="assets/IntelligentSystems/conditionalprob1.png" /></p>
<p>What is the probability of drawing a blue marble, and then another blue one? It's a chance of <span><span class="MathJax_Preview">\frac{2}{5}</span><script type="math/tex">\frac{2}{5}</script></span> followed by a chance of <span><span class="MathJax_Preview">\frac{1}{4}</span><script type="math/tex">\frac{1}{4}</script></span> so overall it's <span><span class="MathJax_Preview">\frac{1}{10}</span><script type="math/tex">\frac{1}{10}</script></span></p>
<p><img alt="conditionalprob2" src="assets/IntelligentSystems/conditionalprob2.png" /></p>
<p>Now define event A to be "draw a blue marble first" and define event B to be "draw a blue marble second"</p>
<p>We have just calculated <span><span class="MathJax_Preview">P(A\, and\, B) = P(A) * P(B|A)</span><script type="math/tex">P(A\, and\, B) = P(A) * P(B|A)</script></span></p>
</details>
</details>
<details class="warning"><summary>Bayes' Rule</summary><p>Bayes' rule is defined as</p>
<div>
<div class="MathJax_Preview">P(B|A) = \frac{P(A|B)P(B)}{P(A)}</div>
<script type="math/tex; mode=display">P(B|A) = \frac{P(A|B)P(B)}{P(A)}</script>
</div>
<p>We can derive this rule with the forumlas in conditional probability. Informally this gives us the probability for B given that A is true. We can think of Bayes' rule as</p>
<div>
<div class="MathJax_Preview">P(hypothesis|evidence) = \frac{P(hypothesis)P(evidence|hypothesis)}{P(evidence)}</div>
<script type="math/tex; mode=display">P(hypothesis|evidence) = \frac{P(hypothesis)P(evidence|hypothesis)}{P(evidence)}</script>
</div>
<p>We can caclulate the probability of our hypothesis based on some evidence which are events we have observed already</p>
<details class="example"><summary>Monty hall problem</summary><p>There are three doors, two hides goats the other hides a car</p>
<ul>
<li>We choose door 1</li>
<li>The host opens door 2 and there is a goat</li>
<li>We can now stay with door 1 or swap to door 3</li>
<li><span><span class="MathJax_Preview">P(win)</span><script type="math/tex">P(win)</script></span> if we stay with door 1 is <span><span class="MathJax_Preview">\frac{1}{3}</span><script type="math/tex">\frac{1}{3}</script></span> as we made our choice when there were 3 doors</li>
<li>What if we swap to door 3?</li>
<li><span><span class="MathJax_Preview">P(B) = \sum_{a} P(B|A = a)P(A=a)</span><script type="math/tex">P(B) = \sum_{a} P(B|A = a)P(A=a)</script></span> (see total probability)</li>
<li>Let B = win and A = car behind door 1 (lose)</li>
<li><span><span class="MathJax_Preview">P(win) = P(win | door1) * P(door1) + P(win | ¬door1) * P(¬door1)</span><script type="math/tex">P(win) = P(win | door1) * P(door1) + P(win | ¬door1) * P(¬door1)</script></span></li>
<li><span><span class="MathJax_Preview">P(win | door1) = 0</span><script type="math/tex">P(win | door1) = 0</script></span> we switched to door 3!</li>
<li><span><span class="MathJax_Preview">P(door1) = \frac{1}{3}</span><script type="math/tex">P(door1) = \frac{1}{3}</script></span> 1 car 2 goats so 1 in 3</li>
<li><span><span class="MathJax_Preview">P(win | ¬door1) = 1</span><script type="math/tex">P(win | ¬door1) = 1</script></span> if not behind door1 it must be behind door3</li>
<li><span><span class="MathJax_Preview">P(¬door1) = \frac{2}{3}</span><script type="math/tex">P(¬door1) = \frac{2}{3}</script></span> as door2 has been opened by the host</li>
<li><span><span class="MathJax_Preview">P(win) = 0 * \frac{1}{3} + 1 * \frac{2}{3} = \frac{2}{3}</span><script type="math/tex">P(win) = 0 * \frac{1}{3} + 1 * \frac{2}{3} = \frac{2}{3}</script></span></li>
</ul>
</details>
</details>
<details class="question"><summary>Independence</summary><p>Two random variables are independent if their joint probability is the product of their probabilities</p>
<div>
<div class="MathJax_Preview">P(A\, and\, B) = P(A) * P(B)</div>
<script type="math/tex; mode=display">P(A\, and\, B) = P(A) * P(B)</script>
</div>
<p>Therefore the conditional probabilities of A and B are equal to the probability of the event itself!</p>
<div>
<div class="MathJax_Preview">P(A|B) = \frac{P(A\, and\, B)}{P(B)} = \frac{P(A) * P(B)}{P(B)} = P(A) </div>
<script type="math/tex; mode=display">P(A|B) = \frac{P(A\, and\, B)}{P(B)} = \frac{P(A) * P(B)}{P(B)} = P(A) </script>
</div>
<div>
<div class="MathJax_Preview">P(B|A) = P(B)</div>
<script type="math/tex; mode=display">P(B|A) = P(B)</script>
</div>
</details>
<h4 id="bayesian-belief-update">Bayesian Belief Update<a class="headerlink" href="#bayesian-belief-update" title="Permanent link">⇦</a></h4>
<p>In bayesian reasoning we always keep a belief model when we make our decisions</p>
<ul>
<li>We use probabilities to capture uncertainty in our knowledge</li>
<li>We also need to update our belief model after we have seen some more evidence</li>
</ul>
<details class="question"><summary>How do we update our belief after each observation?</summary><div>
<div class="MathJax_Preview">P(model|observation) = \frac{P(model)P(observation|model)}{P(observation)}</div>
<script type="math/tex; mode=display">P(model|observation) = \frac{P(model)P(observation|model)}{P(observation)}</script>
</div>
<p>We know all the probabilities on the right side</p>
<ul>
<li><span><span class="MathJax_Preview">P(model)</span><script type="math/tex">P(model)</script></span> is called the <strong>prior</strong>. It is the probability that the model is true without taking any observation into account.</li>
<li><span><span class="MathJax_Preview">P(observation|model)</span><script type="math/tex">P(observation|model)</script></span> is called the <strong>likelihood</strong>. It is the probability that the observed event occurs given that the model is true.</li>
<li><span><span class="MathJax_Preview">P(observation)</span><script type="math/tex">P(observation)</script></span> is the probabilty that the observation occurs in general.</li>
</ul>
<p>We want to calculate the left hand side</p>
<ul>
<li><span><span class="MathJax_Preview">P(model|observation)</span><script type="math/tex">P(model|observation)</script></span> is call the <strong>posterior</strong>. It is the probability that the model is true given that we have seen the observation.</li>
</ul>
</details>
<h4 id="complex-knowledge-representation">Complex Knowledge Representation<a class="headerlink" href="#complex-knowledge-representation" title="Permanent link">⇦</a></h4>
<p>So far we only considered a simple correlation between probabilities but what if we have a much more complicated network of correlations? How can we apply inference in complex networks (ie how can we apply Baye's rule) there?</p>
<details class="warning"><summary>Joint Distribution</summary><p>The simplest way to do inference is to look at the joint distribution of the probability values</p>
<p>Here is an example from an employment survey</p>
<table>
<thead>
<tr>
<th>Male</th>
<th>Long hours</th>
<th>Rich</th>
<th>Probability</th>
</tr>
</thead>
<tbody>
<tr>
<td>T</td>
<td>T</td>
<td>T</td>
<td>0.13</td>
</tr>
<tr>
<td>T</td>
<td>T</td>
<td>F</td>
<td>0.11</td>
</tr>
<tr>
<td>T</td>
<td>F</td>
<td>T</td>
<td>0.10</td>
</tr>
<tr>
<td>T</td>
<td>F</td>
<td>F</td>
<td>0.34</td>
</tr>
<tr>
<td>F</td>
<td>T</td>
<td>T</td>
<td>0.01</td>
</tr>
<tr>
<td>F</td>
<td>T</td>
<td>F</td>
<td>0.04</td>
</tr>
<tr>
<td>F</td>
<td>F</td>
<td>T</td>
<td>0.02</td>
</tr>
<tr>
<td>F</td>
<td>F</td>
<td>F</td>
<td>0.25</td>
</tr>
</tbody>
</table>
<p>Now we can ask questions like "What is the probability that the person is rich?"</p>
<ul>
<li>We need to calculate the total probability of <span><span class="MathJax_Preview">P(Rich)</span><script type="math/tex">P(Rich)</script></span></li>
<li>As we have the table we can sum up all probabilities where <span><span class="MathJax_Preview">Rich = True</span><script type="math/tex">Rich = True</script></span></li>
<li><span><span class="MathJax_Preview">P(Rich) = 0.13 + 0.10 + 0.01 + 0.02 = 0.26</span><script type="math/tex">P(Rich) = 0.13 + 0.10 + 0.01 + 0.02 = 0.26</script></span></li>
</ul>
<p>What about the probability a person works for long hours given that he is a male?</p>
<ul>
<li>We need to calculate <span><span class="MathJax_Preview">P(Longhours | Male)</span><script type="math/tex">P(Longhours | Male)</script></span></li>
<li>This is equal to <span><span class="MathJax_Preview">\frac{P(Longhours\, and\, Male)}{P(Male)}</span><script type="math/tex">\frac{P(Longhours\, and\, Male)}{P(Male)}</script></span></li>
<li><span><span class="MathJax_Preview">P(Longhours\, and\, Male) = 0.13 + 0.11 = 0.24</span><script type="math/tex">P(Longhours\, and\, Male) = 0.13 + 0.11 = 0.24</script></span></li>
<li><span><span class="MathJax_Preview">P(Male) = 0.13 + 0.11 + 0.10 + 0.34 = 0.68</span><script type="math/tex">P(Male) = 0.13 + 0.11 + 0.10 + 0.34 = 0.68</script></span></li>
<li><span><span class="MathJax_Preview">P(Longhours | Male) = 0.24 / 0.68 = 0.35</span><script type="math/tex">P(Longhours | Male) = 0.24 / 0.68 = 0.35</script></span></li>
</ul>
<div class="admonition danger">
<p class="admonition-title"> </p>
<p>We can do any inference from joint distribution however <strong>this does not scale well in practice</strong>. For example if we had <strong>30 variables</strong> we would need a table with <strong>2<sup>30</sup> entries</strong>!</p>
<p>This also does not account for independant variables which we can discard completely!</p>
</div>
</details>
<details class="warning"><summary>Bayesian Networks</summary><p>We can graphically represent the network like this</p>
<p><img alt="bayesiannetwork1" src="assets/IntelligentSystems/bayesiannetwork1.png" /></p>
<ul>
<li><strong>S</strong> = studied for the exam</li>
<li><strong>M</strong> = lecturer is in a good mood</li>
<li><strong>H</strong> = high marks</li>
<li>Every circle (node) is a random variable</li>
<li>Every directed edge from node A to B means B depends on A</li>
<li>Therefore H depends on both S and M</li>
<li>There is no directed edge between S and M, they are <strong>independent</strong></li>
</ul>
<p>What is we wanted to calculate "Given high marks, what is the probability that the lecturer was in a good mood?"</p>
<ul>
<li>This is <span><span class="MathJax_Preview">P(M|H) = \,?</span><script type="math/tex">P(M|H) = \,?</script></span></li>
<li>We will use Bayes' rule <span><span class="MathJax_Preview">P(M|H) = \frac{P(M)P(H|M)}{P(H)}</span><script type="math/tex">P(M|H) = \frac{P(M)P(H|M)}{P(H)}</script></span></li>
<li>We know that <span><span class="MathJax_Preview">P(M) = 0.3</span><script type="math/tex">P(M) = 0.3</script></span> but we don't know <span><span class="MathJax_Preview">P(H)</span><script type="math/tex">P(H)</script></span> or <span><span class="MathJax_Preview">P(H|M)</span><script type="math/tex">P(H|M)</script></span> yet</li>
<li><span><span class="MathJax_Preview">P(H|M)\, = P(H | M\, and\, S) * P(S) + P(H | M\, and\, ¬S) * P(¬S)\\\qquad\qquad = 0.9 * 0.8 + 0.5 * 0.2\\\qquad\qquad = 0.82</span><script type="math/tex">P(H|M)\, = P(H | M\, and\, S) * P(S) + P(H | M\, and\, ¬S) * P(¬S)\\\qquad\qquad = 0.9 * 0.8 + 0.5 * 0.2\\\qquad\qquad = 0.82</script></span></li>
<li><span><span class="MathJax_Preview">P(H) = P(H| M\, and\, S) * P(M\, and\, S)\\\qquad\qquad + P(H | M\, and\, ¬S) * P(M\, and\, ¬S)\\\qquad\qquad + P(H | ¬M\, and\, S) * P(¬M\, and\, S)\\\qquad\qquad + P(H | ¬M\, and\, ¬S) * P(¬M\, and\, ¬S)</span><script type="math/tex">P(H) = P(H| M\, and\, S) * P(M\, and\, S)\\\qquad\qquad + P(H | M\, and\, ¬S) * P(M\, and\, ¬S)\\\qquad\qquad + P(H | ¬M\, and\, S) * P(¬M\, and\, S)\\\qquad\qquad + P(H | ¬M\, and\, ¬S) * P(¬M\, and\, ¬S)</script></span></li>
<li>As we know M and Sare independant we know their joint probability is just their product</li>
<li><span><span class="MathJax_Preview">P(H) = 0.9 * 0.3 * 0.8 \\\qquad\qquad + 0.5 * 0.3 * 0.2 \\\qquad\qquad + 0.4 * 0.7 * 0.8 \\\qquad\qquad + 0.05 * 0.7 * 0.2 \\\quad\quad\;\; = 0.216 + 0.03 + 0.224 + 0.007 \\\quad\quad\;\; = 0.477</span><script type="math/tex">P(H) = 0.9 * 0.3 * 0.8 \\\qquad\qquad + 0.5 * 0.3 * 0.2 \\\qquad\qquad + 0.4 * 0.7 * 0.8 \\\qquad\qquad + 0.05 * 0.7 * 0.2 \\\quad\quad\;\; = 0.216 + 0.03 + 0.224 + 0.007 \\\quad\quad\;\; = 0.477</script></span></li>
<li>Now we can finally calculate <span><span class="MathJax_Preview">P(M|H)</span><script type="math/tex">P(M|H)</script></span>!</li>
<li><span><span class="MathJax_Preview">P(M|H) = \frac{P(M)P(H|M)}{P(H)} = \frac{0.3 * 0.82}{0.477} = 0.516</span><script type="math/tex">P(M|H) = \frac{P(M)P(H|M)}{P(H)} = \frac{0.3 * 0.82}{0.477} = 0.516</script></span></li>
<li>The probability the lecturer was in a good mood, given high marks, is about 52%</li>
</ul>
<details class="warning"><summary>Properties of Bayesian Networks</summary><ul>
<li>Must be directed acyclic graphs</li>
<li>Major efficieny is that we have economized on memory</li>
<li>Easier for humans to interpret</li>
</ul>
<p><strong>Why is it good to use Bayesian Networks?</strong></p>
<ul>
<li>It captures uncertainty of our knowledge about the environ in a very elegant and simple way</li>
<li>We can integrate our prior knowledge into the reasoning by using the prior distribution (which represents our prior knowledge)</li>
<li>We can always update our belief about the world by using Baye's Theorem</li>
</ul>
<p><strong>Why is it bad to use Bayesian Networks?</strong></p>
<ul>
<li>If we use a wrong prior then it will be difficult to get correct answers</li>
<li>The calculation includes integrals and summing over all possible situations (which is typically computationally very expensive!)</li>
</ul>
</details>
</details>
</details>
<details class="info"><summary>Descision Making</summary><h3 id="descision-making">Descision Making<a class="headerlink" href="#descision-making" title="Permanent link">⇦</a></h3>
<p>So far we have looked at classification and updating our belief model. Now we will look into actual decision making and updating our decision making policy. We will do this by using 2 simple models.</p>
<details class="question"><summary>Multi-armed bandits (bandit theory)</summary><h4 id="bandit-theory">Bandit Theory<a class="headerlink" href="#bandit-theory" title="Permanent link">⇦</a></h4>
<p>Suppose we want to outsouce a robbery. There are three different guys from which we can choose one, but we don't know how effective they are. <em>The expected reward per robbery is 50 for the first guy, -10 for the second guy and 30 for the third guy.</em> But remember <strong>we don't know that</strong>. The expected reward is an average value, meaning that they won't always achieve exactly this value. Therefore our goal is to find out that the first guy is most effective via repeated robbing in multiple rounds. Each round we choose exactly one guy. </p>
<details class="info"><summary>Who should we hire each round?</summary><p>It is called <strong>exploration</strong> when we just try out different guys and record their performance</p>
<table>
<thead>
<tr>
<th>guy 1</th>
<th>guy 2</th>
<th>guy 3</th>
</tr>
</thead>
<tbody>
<tr>
<td>50</td>
<td>-10</td>
<td>30</td>
</tr>
<tr>
<td>27</td>
<td>-3</td>
<td>41</td>
</tr>
<tr>
<td>69</td>
<td>4</td>
<td>22</td>
</tr>
<tr>
<td>55</td>
<td>-17</td>
<td>31</td>
</tr>
<tr>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr>
<td>72</td>
<td>-6</td>
<td>33</td>
</tr>
<tr>
<td>-----</td>
<td>average</td>
<td>------</td>
</tr>
<tr>
<td>51.3</td>
<td>-8.6</td>
<td>29.5</td>
</tr>
</tbody>
</table>
<p>We will eventually get to their expected values earlier however <strong>there is a limit to the number of rounds!</strong> We also want to find the best guy as soon as possible so that we can maximise our total reward!</p>
<p>It is called <strong>exploitation</strong> when we always choose the one which we think is best. There is a trade-off between exploration and exploitation.</p>
</details>
<details class="warning"><summary>If we focus too much on exploration</summary><ul>
<li>We can <strong>accurately estimate</strong> each guy's expected performance</li>
<li>But it will take <strong>too much time</strong>, and we'll end up with a <strong>low total reward</strong></li>
</ul>
</details>
<details class="warning"><summary>If we focus too much on exploitation</summary><ul>
<li>We can focus on <strong>maximising the total reward</strong></li>
<li>But we <strong>might miss</strong> a bandit with <strong>a higher reward</strong></li>
</ul>
</details>
<h4 id="multi-armed-bandit-mab-model">Multi-armed Bandit (MAB) Model<a class="headerlink" href="#multi-armed-bandit-mab-model" title="Permanent link">⇦</a></h4>
<p>The key challenge is to efficiently balance exploration and exploitation such that we maximise the total reward.</p>
<p>There are multiple arms with different distributions (probabilities that we win). We don't know the distribution of the arms. Here is an example model with three arms.</p>
<p><img alt="mabmodel" src="assets/IntelligentSystems/mabmodel.png" /></p>
<p>We are going to "play" with the arms for multiple rounds, where we can select one in each round. Our objective is to maximise the total reward. How explorative or exploitative should we be? The Multi-armed Bandit Model is the simplest model that captures the dilemma of exploration vs exploitation. How can we solve the MAB problem?</p>
<ul>
<li>We dont have knowledge about the expected reward values</li>
<li>What is the optimal total value?</li>
<li>Can we achieve the optimal total value?</li>
</ul>
<p>Our goal is to design algorithms that are close to the optimum as much as possible (good approximation). There are two different approaches to solving this.</p>
<details class="warning"><summary>Epsilon-first approach</summary><ul>
<li>Suppose the number of rounds <strong>T</strong> is fixed</li>
<li>Choose an epsilon value <strong>0 &lt; epsilon &lt; 1</strong> (typically between 0.05 and 0.2)</li>
<li>In the first <strong>epsilon*T</strong> rounds we <strong>only do exploration</strong> by pulling the arms in a round robin manner</li>
<li><strong>After</strong> that we do <strong>exploitation</strong> by choosing the arm with the highest average reward value</li>
</ul>
<div class="admonition example">
<p>Suppose <strong>epsilon is 0.1</strong> and <strong>T is 100</strong></p>
<ul>
<li>In the first 10 rounds (0.1 * 100) we pull all three arms in a round robin manner</li>
<li>After the first 10 rounds we calculate the arm with the highest average reward</li>
<li>In the last 90 rounds (0.9*100) we pull this arm only</li>
</ul>
</div>
</details>
<details class="warning"><summary>Epsilon-greedy appraoch</summary><ul>
<li>Choose an epsilon value <strong>0 &lt; epsilon &lt; 1</strong> (typically between 0.05 and 0.2)</li>
<li>Pull the arm with the <strong>current best average</strong> reward value with <strong>probability 1-epsilon</strong> </li>
<li>Pull one of the other arms <strong>uniformly at random</strong></li>
<li>Repeat this for each round</li>
</ul>
<div class="admonition example">
<p>Suppose <strong>epsilon is 0.1</strong></p>
<ul>
<li>We pull the arm with the current best average reward 90% of the time</li>
<li>We pull a random arm 10% of the time</li>
</ul>
</div>
</details>
<table>
<thead>
<tr>
<th>Epsilon-first</th>
<th>Epsilon-greedy</th>
</tr>
</thead>
<tbody>
<tr>
<td>✔️Typically very good when <strong>T</strong> is <strong>small</strong></td>
<td>✔️Typically efficient when <strong>T</strong> is sufficiently <strong>large</strong></td>
</tr>
<tr>
<td>❗Needs to know T in advance</td>
<td>❗Slow convergance at start</td>
</tr>
<tr>
<td>❗Sensitive to epsilon value</td>
<td>❗Sensitive to epsilon value</td>
</tr>
</tbody>
</table>
<h4 id="other-algorithms">Other Algorithms<a class="headerlink" href="#other-algorithms" title="Permanent link">⇦</a></h4>
<p>There are also other algorithms, not just epsilon-first and epsilon-greedy. One example is <strong>Upper Confidence Bound</strong> (UCB) which combines explorations and exploitation within each single round in a very clever way.</p>
<p>Another example is <strong>Thompson-sampling</strong> which involves maintaining a belief distribution about the true expect reward of each arm using Bayes' Theorem. Randomly sample from each of these believes then choose the arm with the highest sample. We repeat this each round.</p>
<details class="warning"><summary>Performance of Bandit Algorithms and Regret</summary><ul>
<li>How can we compare the different algorithms?</li>
<li>How can we measure the goodness of an algorithm in general?</li>
<li>Can we always achieve the optimal solution (best possible)?</li>
<li><strong>Spoilers</strong> We cannot achieve the best possible!</li>
</ul>
<p>Our aim is to design algorithms that have close performance to that of the best possible. One measurement to quantify the performance is called regret. <strong>Regret is the difference between the performance of an algorithm with that of the best possible for a certain number of steps taken</strong>.</p>
<p>Let's go back to the robbery example</p>
<ul>
<li>We have <strong>3 arms</strong> and we pull <strong>100 times</strong></li>
<li>Arm 1, 2 and 3 have the expected rewards 50, -10 and 30</li>
<li><strong>Remember we aren't supposed to know that!</strong></li>
<li>The <strong>best possible algorithm</strong> would choose arm 1 all 100 times, therefore the total reward would be <strong>5000</strong></li>
<li>Suppose <strong>our algorithm chooses</strong> arm 2 20 times and arm 3 10 times and for the rest it pulls arm 1. The total reward is -10*20 + 30*10 + 50*70 = <strong>3600</strong></li>
<li>The difference (ie <strong>regret</strong>) is 5000 - 3600 = <strong>1400</strong></li>
<li>We can then say the regret for this algorithm in 100 time steps is 1400</li>
</ul>
<p>Does there exist a no-regret algorithm? Hold-on! A <strong>no-regret algorithm</strong> is an algorithm where its <strong>average regret converages to 0</strong> when the number of <strong>time steps approaches infinity</strong>. This indicates that, on average, the algorithm always pulls the best arm. This also indicates that, on average, a no-regret algorithm will start to behave like the optimal (best) algorithm. That's exactly what we want!</p>
</details>
<details class="tip"><summary>Extensions of the Multi-armed Bandit</summary><p>There are some extensions to the multi-armed bandit model. </p>
<ul>
<li><strong>budget-limited bandit</strong> we have to pay a cost to pull an arm with a total budget limit</li>
<li><strong>dueling bandits</strong> we choose 2 bandits at each round and we only check which one is better but not the actual reward values</li>
<li><strong>best-arm bandit</strong> we aim to identify the best arm, we do this by pure learning (only exploration no exploitation)</li>
</ul>
</details>
</details>
<details class="question"><summary>Markov decision process (reinforcement learning)</summary><h4 id="reinforcement-learning">Reinforcement Learning<a class="headerlink" href="#reinforcement-learning" title="Permanent link">⇦</a></h4>
<p>How do humans learn? They <strong>learn from experiences</strong>. </p>
<p>Let's start with an example problem, an agent wants to find the way out from a maze.</p>
<p><img alt="reinforcementmaze" src="assets/IntelligentSystems/reinforcementmaze.png" /></p>
<p>At each time step the agent</p>
<ul>
<li>Chooses a direction</li>
<li>Makes a step</li>
<li>Checks whether it's the exit</li>
</ul>
<p>This is a standard search problem! Let's set a new goal for our problem. The agent wants to find the <strong>shortest path</strong> from A (entrance) to B (exit). What we want to have is a policy of behaviour, at each situation it will tell the agent what to do.</p>
<details class="info"><summary>Feedback</summary><ul>
<li>In reinforcement learning, the agent <strong>receives feedback</strong> on how good the chosen action was</li>
<li>There is continuous feedback until the goal is reached</li>
<li>It is inbetween supervised and unsupervised learning</li>
<li>The agent repeatedly interacts with the environment</li>
<li>It gets some feedback, <strong>positive or negative</strong>, hence reinforcement</li>
<li>It is a learning problem because we try to find a good policy based on the feedback</li>
<li>A policy is a set of rules that tells the agent what to do at each state</li>
<li>How do we know what actions lead us to victory (our goal)?</li>
<li>What about those that made us lose the game?</li>
<li>How can we measure which action is the best to take at each time step?</li>
<li>We need to be able to evaluate the actions we take and the states we are in</li>
</ul>
</details>
<h4 id="states-actions-and-rewards">States, Actions and Rewards<a class="headerlink" href="#states-actions-and-rewards" title="Permanent link">⇦</a></h4>
<p>Let's define states, actions and rewards</p>
<ul>
<li>We can think about the world as a set of states, there are <strong>good</strong> and <strong>bad</strong> states (less or more ideal)</li>
<li>With an action we move from one state to another</li>
<li>The reward is the feedback of the environment, it measures how good an action was</li>
</ul>
<p>We want to maximise the sum of collected rewards over time.</p>
<details class="warning"><summary>Temporal Difference Learning</summary><p>Suppose we try to find the shortest path for some maze. </p>
<ul>
<li>There are six states where one is the terminal state (exit state).</li>
</ul>
<p><img alt="temporaldif" src="assets/IntelligentSystems/temporaldif.png" /></p>
<ul>
<li>We will start at state A and try to reach state F, which gives a reward of <strong>100</strong></li>
<li>All other states, initially, have a reward of <strong>0</strong></li>
<li>We want to <strong>maximise the rewards</strong> over time, which is equivalent to finding the shortest path</li>
<li>At the beginning we don't have any prior knowledge so we have to <strong>start with a really simple policy</strong>, just <strong>randomly move at each state</strong>!</li>
<li>Later when they have reward values we will always move to the state with the highest value</li>
<li>Sometime later we eventually arrive at F where we receive a reward of 100</li>
<li>What was the last state before F? Surely that state must be pretty good too</li>
<li>We then update the value of that state to be good</li>
<li>If we repeat this process we are performing <strong>temporal difference learning</strong></li>
<li>We maintain the value <strong>V</strong> of each state, they represent how good a state is</li>
</ul>
<p><img alt="temporaldif2" src="assets/IntelligentSystems/temporaldif2.png" /></p>
<p><strong>How are we going to update our esitmate of each state's value?</strong> </p>
<ul>
<li>Immediate reward is important, if moving into a certain state gives us a reward or punishment we need to record that straight away! </li>
<li>But future reward is important too. A state may give us nothing now but we still like it if it is linked to future reinforcement</li>
<li>It is also important not to let any single learning experience change our opinion too much</li>
<li>We want to change our V estimates gradually to allow the long-run picture to emerge</li>
<li>We need a <strong>learning rate</strong></li>
</ul>
<p>The formula for temporal difference learning combines these factors</p>
<ul>
<li><strong>Current reward</strong></li>
<li><strong>Future reward</strong></li>
<li><strong>Learning rate</strong></li>
</ul>
<p>Suppose we go from state <strong>i</strong> to state <strong>j</strong> then we will update the value of state <strong>i</strong> </p>
<p><img alt="learningrate" src="assets/IntelligentSystems/learningrate.png" /></p>
<p><strong>What is the learning rate?</strong></p>
<ul>
<li>It determines to what extent newly acquired information overrides old information</li>
<li>If we choose <strong>a=0</strong> then the agent <strong>learns nothing</strong></li>
<li>If we choose <strong>a=1</strong> then the agen only considers the most recent information</li>
<li>Ideally we choose some value between 0 and 1</li>
</ul>
<p>But wait, there is no negative feedback! <strong>Eventually all states will reach a max good value!</strong> We need some way to distinguish paths that require less moves. To do this we use a <strong>discount factor</strong> with a value between 0 and 1.</p>
<p><img alt="learningrate2" src="assets/IntelligentSystems/learningrate2.png" /></p>
</details>
<details class="warning"><summary>Q-Learning</summary><p>In some cases we also need to learn the outcomes of the actions. </p>
<p>This is the case if we don't know which actions will take us to which state. In other words we want to learn the quality of taking each action at each state. This is called the <strong>Q value</strong> for a specific action and a specific state. </p>
<p>The <strong>Q value</strong> for <strong>state i</strong> and <strong>action k</strong> which leads to <strong>state j* with </strong>actions x** is calculated by</p>
<p><img alt="qcalculation" src="assets/IntelligentSystems/qcalculation.png" /></p>
</details>
<h4 id="markov-decision-processes-mdp-and-markov-chain">Markov Decision Processes (MDP) and Markov Chain<a class="headerlink" href="#markov-decision-processes-mdp-and-markov-chain" title="Permanent link">⇦</a></h4>
<p>In real world problems the state transitions are often <strong>stochastic</strong> (random)</p>
<p>Here is an example</p>
<ul>
<li>You as a student want to graduate within 4 years</li>
<li>Obviously there are actions you can take to achieve this goal</li>
<li>However there will be some random chance which plays some role too</li>
<li>We can model the probabilities of taking a certain action in a <strong>Markov chain</strong></li>
</ul>
<p><img alt="markovchain" src="assets/IntelligentSystems/markovchain.png" /></p>
<ul>
<li>Suppose the current state is state <strong>3</strong></li>
<li>If we take the action there is a <strong>50% probability</strong> that we go to state <strong>1</strong>, and a <strong>50% probability</strong> we go to state <strong>4</strong></li>
<li>If the probability of arriving at the next state <strong>only</strong> depends on the <strong>current state and action</strong> (not any previous states) then the process has a <strong>Markov property</strong></li>
<li>We can also model the probabilities in an <strong>n</strong> by <strong>n</strong> matrix <span><span class="MathJax_Preview">P</span><script type="math/tex">P</script></span> with <span><span class="MathJax_Preview">P_{ij}</span><script type="math/tex">P_{ij}</script></span> being the probability of making a transition from state <strong>i</strong> to state <strong>j</strong></li>
</ul>
<p><img alt="markovmatrix" src="assets/IntelligentSystems/markovmatrix.png" /></p>
<ul>
<li>After we have define the matrix we can also calculate the probability of being in each state after <strong>k</strong> steps</li>
<li>In our example we take state <strong>1</strong> as start state and model this vector <span><span class="MathJax_Preview">b = (1, 0, 0, 0)</span><script type="math/tex">b = (1, 0, 0, 0)</script></span></li>
<li>Now to caculate the probabilities of being in each state after <strong>k</strong> steps we will simply calculate <span><span class="MathJax_Preview">b^{t}P^{k}</span><script type="math/tex">b^{t}P^{k}</script></span></li>
<li>We will get a new column vector with the respective probabilities in each state</li>
</ul>
<p><img alt="markovmatrix2" src="assets/IntelligentSystems/markovmatrix2.png" /></p>
<div class="admonition tip">
<p class="admonition-title"> </p>
<p>We can also apply <strong>temporal difference</strong> and <strong>Q-Learning</strong> to <strong>MDP</strong> but we need to add the probability for the state transition</p>
<p><img alt="markovlearning" src="assets/IntelligentSystems/markovlearning.png" /></p>
</div>
</details>
</details>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="DistributedSystemsAndNetworks.html" title="Distributed Systems & Networks" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Distributed Systems & Networks
              </span>
            </div>
          </a>
        
        
          <a href="ProgrammingLanguageConcepts.html" title="Programming Language Concepts" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Programming Language Concepts
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="assets/javascripts/application.b41f3d20.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:"."}})</script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
    
      
    
  </body>
</html>