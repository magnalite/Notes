{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Home \u21e6","title":"Home"},{"location":"index.html#home","text":"","title":"Home"},{"location":"BodyBuilding.html","text":"Body Building and general nutrition \u21e6 Science of hypertrophy \u21e6 What causes hypertrophy Muscular hypertrophy can be increased through strength training and other short-duration, high-intensity anaerobic exercises . Lower-intensity, longer-duration aerobic exercise generally does not result in very effective tissue hypertrophy; instead, endurance athletes enhance storage of fats and carbohydrates within the muscles, as well as neovascularization (creation of new blood vessels). 1 A positive energy balance, when more calories are consumed rather than burned, is required for anabolism (construction of complex molecules) and therefore muscle hypertrophy. An increased requirement for protein, especially branch chained amino acids , is required for elevated protein synthesis that is seen in athletes training for muscle hypertrophy. 1 Training variables, in the context of strength training, such as frequency, intensity, and total volume also directly affect the increase of muscle hypertrophy . A gradual increase in all of these training variables will yield the muscular hypertrophy. 1 Exercise-induced muscle damage (EIMD) occurs primarily from the performance of unaccustomed exercise, and its severity is modulated by the type, intensity, and/or duration of training. Although concentric and isometric actions contribute to EIMD, the greatest damage to muscle tissue is seen with eccentric exercise, where muscles are forcibly lengthened . 5 Although EIMD can have detrimental short-term effects on markers of performance and pain, it has been hypothesized that the associated skeletal muscle inflammation and increased protein turnover are necessary for long-term hypertrophic adaptions . 5 It is unclear if muscle damage directly causes hypertrophy or if it just happens to coincide with elevated hypertrophy. What is consistent is that exercises which result in muscle damage also result in higher levels of hypertrophy . 5 What rep range is best? \"Training in a lower rep range (1-5 per set) maximizes strength increases , thereby facilitating your ability to use heavier weights during moderate rep training. In this way, you create greater tension in the muscles, spurring better growth. High reps (15-20 per set), on the other hand, help to increase your lactate threshold . By delaying the buildup of lactic acid, you stave off fatigue when training in the \u201chypertrophy range,\u201d (the muscle building range) thus increasing time under tension\u2014another important aspect of the growth process. Bottom line is that optimum muscle development is best achieved by using the full spectrum of rep ranges. Periodize your program so that it is built around a moderate repetition protocol, but you make sure to include training in both the lower and higher rep ranges.\" 3 There are two types of muscle fibre appropriately named type I and type II. \" Type I fibers are maximally stimulated with longer duration sets requiring lower loads, while Type II fibers respond best to short sets with heavy weights .\" 6 While type II fibres seem to provide higher levels of hypertrophy during initial training the potential gains available from type I fibres should not be ignored. \"The idea that you're sacrificing pounds of muscle by ignoring lighter-load training may seem like an exaggeration, but a quick survey of the fiber-type composition of various muscles may change your mind . Granted, fiber-type proportion can vary by individual and is influenced by genetics and training (22), but given that many of the major groups have a substantial percentage of type I fibers \u2013 on average, humans have a roughly equal amount of fast and slow-twitch fibers \u2013 means it may be worth tweaking your approach to optimize slow-twitch fiber growth.\" 6 \" For those who want to maximize their hypertrophic potential, it makes sense to train across the continuum of repetition ranges. While there may be validity to focusing on the so-called \"hypertrophy range\" (6-12 reps), both high (15-20+) and low (1-5) repetition ranges should also be incorporated into your training program .\" 6 How many sets is best? \"Back in the 1970's, Arthur Jones popularized the so-called high-intensity training (HIT, not to be confused with HIIT \u2013 high intensity interval training) approach to building muscle. HIT is based on the premise that only a single set of an exercise is necessary to stimulate growth, provided you train to the point of momentary concentric muscular failure . According to HIT dogma, performing additional sets beyond this first set is superfluous and perhaps even counterproductive to muscle development. Other prominent industry leaders such as Mike Mentzer and Ellington Darden subsequently followed Jones's lead and embraced the HIT philosophy, resulting in a surge in its popularity. To this day, HIT continues to enjoy an ardent following. Now before I get accused of being anti-HIT, I'll readily admit that it's a viable training strategy. There's no denying that it can help build appreciable muscle. And if you're time-pressed, it can provide an efficient and effective workout . That said, if your goal is to maximize muscle development, HIT simply doesn't do the trick . You need a higher training volume. Substantially higher than just one set per exercise. The prevailing body of research consistently shows that multiple set protocols are superior to single set protocols for increasing strength and size . Recent meta-analyses published in The Journal of Strength and Conditioning Research show that multiple set training results in 46% greater increases in strength and 40% greater increases in muscle growth when compared to single-set protocols. Whether the hypertrophic superiority of multiple sets is due to greater total muscle tension, muscle damage, metabolic stress, or some combination of these factors isn't clear. What is clear is that multiple sets are a must if you want to maximize your muscular potential . Problem is, even if you employ multiple sets it's very possible you're still not training with sufficient volume . The optimal number of sets needed to elicit superior growth will vary from person to person and depend on a host of individual factors such as genetics, recuperative ability, training experience, and nutritional status. But individual response is only part of the equation. The size of a given muscle also has relevance. Larger muscle groups such as the back and thighs need a higher volume than the smaller muscles of the arms and calves, which get significant ancillary work during multi-joint exercises . Another important consideration here is the structure of your program. All things being equal, training with a split routine allows for a greater daily training volume per muscle group versus a total body routine. And if you follow a training split, the composition of your split will influence training daily volume (a 3-day split allows for a greater volume per muscle group compared with a 2-day split). Accordingly, training volume is best determined on a weekly basis as opposed to a single session. Whatever your target weekly volume, optimal results are achieved by taking a periodized approach where the number of sets are strategically manipulated over the course of a training cycle. Understand that repeatedly training with high volumes will inevitably lead to overtraining . In fact, evidence shows that volume has an even greater correlation with overtraining than intensity . Only by embracing periodization can you reap the benefits of a high training volume while avoiding the dreaded overtrained state . Here's a periodized strategy that I've found to be highly effective. Let's say you've determined that your maximum weekly volume should entail performing 18-20 sets per muscle group. Focus on a three-month mesocycle where you target 8-10 sets a week the first month, 14-16 sets the second month, and then culminate with an overreaching cycle in the final month where you perform 18-20 sets per week. Follow this with a brief period of unloading or active recovery to facilitate restoration and rejuvenation. Given that it generally takes one to two weeks for the full effects of supercompensation to manifest after completion of an overreaching cycle, you should realize optimal muscular gains sometime during the restorative period .\" 4 Do I need variety in exercises? \"Changing up your exercise selection has a couple of important benefits from a mass-building standpoint. For one, it helps to prevent the so-called \u201crepeated-bout effect\u201d whereby muscles become accustomed to the continual use of the same movements, making them increasingly resistant to trauma. Staving off such accommodation allows for greater structural perturbations to muscle fibers. That might sound like a bunch of confusing science to some of you, but what it all means is that changing exercises can facilitate increased growth . What\u2019s more, muscle fibers don\u2019t necessarily span the entire length of a fiber and are often innervated by different nerve branches . Thus, exercise variety alters recruitment patterns in the musculature, ensuring optimal stimulation of all fibers.\" 3 \"Employ a diverse selection of exercises over the course of your training cycle. This can be accomplished by switching around modalities, training angles, planes of movement, and even your hand and foot spacing. ( For instance, on dumbbell curls, think about holding the handle with your pinky against one end of the bell, and on the next set perform with your thumb against the bell. That slight shift will work your biceps in different ways .)\" 3 Isolation vs Compound \"Multi-joint exercises involve large amounts of muscle and therefore are highly efficient for packing on mass. Alternatively, single-joint exercises allow for greater targeting of individual muscles (or even portions of muscles), enhancing overall growth and symmetry.\" 3 \"Structure your routine so that it is comprised of a combination of multi- and single-joint exercises. As a general rule, every workout should contain at least one or two 'big lifts' and a single-joint move .\" 3 Do I need more than just straight sets? \"There\u2019s nothing \u201cwrong\u201d with the basic approach; straight sets can and perhaps even should form the foundation of your routine. But it\u2019s also good to mix things up a bit with some specialized techniques. Supersets (performing one exercise followed immediate by another exercise without rest), drop sets (performing a set to muscular failure with a given load and then immediately reducing the load and continuing to train until subsequent failure) and heavy negatives (performing eccentric actions\u2014the lowering of a weight\u2014at a weight greater than concentric 1-repetition maximum) can be excellent additions to a mass-building routine. They help to induce greater metabolic stress and structural perturbations that can take your muscle growth to new heights. These strategies are particularly effective for advanced lifters who need to \u201cshock\u201d their body to spur greater growth . Selectively add specialized techniques such as supersets, drop sets, and heavy negatives into your routine. A word of caution: these techniques should be considered advanced training strategies. Their fatiguing nature increases the risk for overtraining , and it is therefore wise to limit their use to no more than a few microcycles over the course of a periodized program.\" 3 What about cardio? \"A goal of many lifters is to increase muscle development while simultaneously reducing body fat levels. In an attempt to accelerate fat loss, cardio is frequently ramped up while performing intense resistance training. Adding some aerobic training to a muscle-building routine isn\u2019t necessarily a bad thing . Overdoing it, however, certainly is . You see, the signaling pathways for resistance training and aerobic training are contradictory. Some researchers have coined the term \u201cAMPK-PKB switch\u201d whereby aerobic training promotes catabolic processes (AMPK is involved in pathways associated with protein breakdown, which for your sake can be considered \u201cmuscle wasting\u201d) and resistance training promotes anabolic processes (PKB is involved in pathways associated with protein synthesis, or for you, \u201cmuscle gaining\u201d). If your goal is to maximize muscle, keep cardio at moderate levels. How much is too much? It ultimately depends on the individual, as some can tolerate more than others. A general guideline is to limit steady state cardio to no more than about 3 or 4 weekly bouts lasting 30 to 40 minutes .\" 3 Metabolic resistance training (burn fat and gain muscle) \"MRT, a.k.a. \"metabolic resistance training,\" might as well be called \"madman training.\" It's no-holds-barred, haul-ass, maximum-effort, build-muscle, heave-weight, torch-fat, absolutely insane huff-n-puff training. It'll spike your metabolism, crush calories like beer cans, lift your lactate threshold, boost your ability to make muscle, and maximize your body's capacity for change. \" 7 \"MRT works by heightening the metabolic \"cost\" of exercise. This might sound geeky... until you try it. Whereas traditional resistance training might tap 25 or 30% of the body's \"change capacity,\" MRT can maximize your potential for change and unleash metabolic forces that work all day and night. By maximizing your body's change capacity, you can improve 50%\u2014not 25 or 30%\u2014in only 6 weeks . Even better, MRT spreads improvement across multiple desired targets. Basically, when properly integrated into a periodized-training scheme, MRT can help you build muscle, burn fat and gain strength at the same time.\" 7 Basically do whole body workouts really fast with minimal rest between sets and taking sets to muscular failure. Read source 7 for more info. Nutrition \u21e6 General protein intake 1.6-2.2g/kg (varied) protein daily is considered sufficient ; there is no detriment to having more (kidney problems only occur in people with already compromised kidney function) 2 Do not over consume BCAA\u2019s, this may outcompete essential amino acids for absorption 2 BCAA\u2019s are not necessary if overall protein intake is sufficiently varied/complete 2 Anabolic window There is insufficient evidence to strongly conclude that immediate consumption of protein after a workout has significant benefits. As long as there is protein available for synthesis (ie from a meal taken before the workout) there should be no hinderence to hypertrohpy. There is an elevated window of protein synthesis for around 2-4 hours post workout however excessive consumption of protein will not significantly affect hypertrophy. This elevated window remains slightly elevated for 24-36 hours after the initial 2-4 hour spike. 8 ? \"Due to the transient anabolic impact of a protein-rich meal and its potential synergy with the trained state, pre- and post-exercise meals should not be separated by more than approximately 3\u20134 hours, given a typical resistance training bout lasting 45\u201390 minutes.\" 8 Misc \u21e6 Sources https://en.wikipedia.org/wiki/Muscle_hypertrophy https://www.youtube.com/watch?v=Uc265c-tLGY (Brad Schoenfeld) https://www.bornfitness.com/muscle-building-mistakes/ (Brad Schoenfeld) https://www.t-nation.com/training/4-reasons-youre-not-gaining-muscle (Brad Schoenfeld) http://www.lookgreatnaked.com/articles/does_exercise_induced_muscle_damage_play_a_role_in_skeletal_muscle_hypertrophy.pdf (Brad Schoenfeld) https://www.t-nation.com/training/light-weights-for-big-gains (Brad Schoenfeld) https://www.bodybuilding.com/content/metabolic-resistance-training-build-muscle-and-torch-fat-at-once.html (Brad Schoenfeld) https://jissn.biomedcentral.com/articles/10.1186/1550-2783-10-5 (Brad Shoenfeld)","title":"Body Building and general nutrition"},{"location":"BodyBuilding.html#body-building-and-general-nutrition","text":"","title":"Body Building and general nutrition"},{"location":"BodyBuilding.html#science-of-hypertrophy","text":"What causes hypertrophy Muscular hypertrophy can be increased through strength training and other short-duration, high-intensity anaerobic exercises . Lower-intensity, longer-duration aerobic exercise generally does not result in very effective tissue hypertrophy; instead, endurance athletes enhance storage of fats and carbohydrates within the muscles, as well as neovascularization (creation of new blood vessels). 1 A positive energy balance, when more calories are consumed rather than burned, is required for anabolism (construction of complex molecules) and therefore muscle hypertrophy. An increased requirement for protein, especially branch chained amino acids , is required for elevated protein synthesis that is seen in athletes training for muscle hypertrophy. 1 Training variables, in the context of strength training, such as frequency, intensity, and total volume also directly affect the increase of muscle hypertrophy . A gradual increase in all of these training variables will yield the muscular hypertrophy. 1 Exercise-induced muscle damage (EIMD) occurs primarily from the performance of unaccustomed exercise, and its severity is modulated by the type, intensity, and/or duration of training. Although concentric and isometric actions contribute to EIMD, the greatest damage to muscle tissue is seen with eccentric exercise, where muscles are forcibly lengthened . 5 Although EIMD can have detrimental short-term effects on markers of performance and pain, it has been hypothesized that the associated skeletal muscle inflammation and increased protein turnover are necessary for long-term hypertrophic adaptions . 5 It is unclear if muscle damage directly causes hypertrophy or if it just happens to coincide with elevated hypertrophy. What is consistent is that exercises which result in muscle damage also result in higher levels of hypertrophy . 5 What rep range is best? \"Training in a lower rep range (1-5 per set) maximizes strength increases , thereby facilitating your ability to use heavier weights during moderate rep training. In this way, you create greater tension in the muscles, spurring better growth. High reps (15-20 per set), on the other hand, help to increase your lactate threshold . By delaying the buildup of lactic acid, you stave off fatigue when training in the \u201chypertrophy range,\u201d (the muscle building range) thus increasing time under tension\u2014another important aspect of the growth process. Bottom line is that optimum muscle development is best achieved by using the full spectrum of rep ranges. Periodize your program so that it is built around a moderate repetition protocol, but you make sure to include training in both the lower and higher rep ranges.\" 3 There are two types of muscle fibre appropriately named type I and type II. \" Type I fibers are maximally stimulated with longer duration sets requiring lower loads, while Type II fibers respond best to short sets with heavy weights .\" 6 While type II fibres seem to provide higher levels of hypertrophy during initial training the potential gains available from type I fibres should not be ignored. \"The idea that you're sacrificing pounds of muscle by ignoring lighter-load training may seem like an exaggeration, but a quick survey of the fiber-type composition of various muscles may change your mind . Granted, fiber-type proportion can vary by individual and is influenced by genetics and training (22), but given that many of the major groups have a substantial percentage of type I fibers \u2013 on average, humans have a roughly equal amount of fast and slow-twitch fibers \u2013 means it may be worth tweaking your approach to optimize slow-twitch fiber growth.\" 6 \" For those who want to maximize their hypertrophic potential, it makes sense to train across the continuum of repetition ranges. While there may be validity to focusing on the so-called \"hypertrophy range\" (6-12 reps), both high (15-20+) and low (1-5) repetition ranges should also be incorporated into your training program .\" 6 How many sets is best? \"Back in the 1970's, Arthur Jones popularized the so-called high-intensity training (HIT, not to be confused with HIIT \u2013 high intensity interval training) approach to building muscle. HIT is based on the premise that only a single set of an exercise is necessary to stimulate growth, provided you train to the point of momentary concentric muscular failure . According to HIT dogma, performing additional sets beyond this first set is superfluous and perhaps even counterproductive to muscle development. Other prominent industry leaders such as Mike Mentzer and Ellington Darden subsequently followed Jones's lead and embraced the HIT philosophy, resulting in a surge in its popularity. To this day, HIT continues to enjoy an ardent following. Now before I get accused of being anti-HIT, I'll readily admit that it's a viable training strategy. There's no denying that it can help build appreciable muscle. And if you're time-pressed, it can provide an efficient and effective workout . That said, if your goal is to maximize muscle development, HIT simply doesn't do the trick . You need a higher training volume. Substantially higher than just one set per exercise. The prevailing body of research consistently shows that multiple set protocols are superior to single set protocols for increasing strength and size . Recent meta-analyses published in The Journal of Strength and Conditioning Research show that multiple set training results in 46% greater increases in strength and 40% greater increases in muscle growth when compared to single-set protocols. Whether the hypertrophic superiority of multiple sets is due to greater total muscle tension, muscle damage, metabolic stress, or some combination of these factors isn't clear. What is clear is that multiple sets are a must if you want to maximize your muscular potential . Problem is, even if you employ multiple sets it's very possible you're still not training with sufficient volume . The optimal number of sets needed to elicit superior growth will vary from person to person and depend on a host of individual factors such as genetics, recuperative ability, training experience, and nutritional status. But individual response is only part of the equation. The size of a given muscle also has relevance. Larger muscle groups such as the back and thighs need a higher volume than the smaller muscles of the arms and calves, which get significant ancillary work during multi-joint exercises . Another important consideration here is the structure of your program. All things being equal, training with a split routine allows for a greater daily training volume per muscle group versus a total body routine. And if you follow a training split, the composition of your split will influence training daily volume (a 3-day split allows for a greater volume per muscle group compared with a 2-day split). Accordingly, training volume is best determined on a weekly basis as opposed to a single session. Whatever your target weekly volume, optimal results are achieved by taking a periodized approach where the number of sets are strategically manipulated over the course of a training cycle. Understand that repeatedly training with high volumes will inevitably lead to overtraining . In fact, evidence shows that volume has an even greater correlation with overtraining than intensity . Only by embracing periodization can you reap the benefits of a high training volume while avoiding the dreaded overtrained state . Here's a periodized strategy that I've found to be highly effective. Let's say you've determined that your maximum weekly volume should entail performing 18-20 sets per muscle group. Focus on a three-month mesocycle where you target 8-10 sets a week the first month, 14-16 sets the second month, and then culminate with an overreaching cycle in the final month where you perform 18-20 sets per week. Follow this with a brief period of unloading or active recovery to facilitate restoration and rejuvenation. Given that it generally takes one to two weeks for the full effects of supercompensation to manifest after completion of an overreaching cycle, you should realize optimal muscular gains sometime during the restorative period .\" 4 Do I need variety in exercises? \"Changing up your exercise selection has a couple of important benefits from a mass-building standpoint. For one, it helps to prevent the so-called \u201crepeated-bout effect\u201d whereby muscles become accustomed to the continual use of the same movements, making them increasingly resistant to trauma. Staving off such accommodation allows for greater structural perturbations to muscle fibers. That might sound like a bunch of confusing science to some of you, but what it all means is that changing exercises can facilitate increased growth . What\u2019s more, muscle fibers don\u2019t necessarily span the entire length of a fiber and are often innervated by different nerve branches . Thus, exercise variety alters recruitment patterns in the musculature, ensuring optimal stimulation of all fibers.\" 3 \"Employ a diverse selection of exercises over the course of your training cycle. This can be accomplished by switching around modalities, training angles, planes of movement, and even your hand and foot spacing. ( For instance, on dumbbell curls, think about holding the handle with your pinky against one end of the bell, and on the next set perform with your thumb against the bell. That slight shift will work your biceps in different ways .)\" 3 Isolation vs Compound \"Multi-joint exercises involve large amounts of muscle and therefore are highly efficient for packing on mass. Alternatively, single-joint exercises allow for greater targeting of individual muscles (or even portions of muscles), enhancing overall growth and symmetry.\" 3 \"Structure your routine so that it is comprised of a combination of multi- and single-joint exercises. As a general rule, every workout should contain at least one or two 'big lifts' and a single-joint move .\" 3 Do I need more than just straight sets? \"There\u2019s nothing \u201cwrong\u201d with the basic approach; straight sets can and perhaps even should form the foundation of your routine. But it\u2019s also good to mix things up a bit with some specialized techniques. Supersets (performing one exercise followed immediate by another exercise without rest), drop sets (performing a set to muscular failure with a given load and then immediately reducing the load and continuing to train until subsequent failure) and heavy negatives (performing eccentric actions\u2014the lowering of a weight\u2014at a weight greater than concentric 1-repetition maximum) can be excellent additions to a mass-building routine. They help to induce greater metabolic stress and structural perturbations that can take your muscle growth to new heights. These strategies are particularly effective for advanced lifters who need to \u201cshock\u201d their body to spur greater growth . Selectively add specialized techniques such as supersets, drop sets, and heavy negatives into your routine. A word of caution: these techniques should be considered advanced training strategies. Their fatiguing nature increases the risk for overtraining , and it is therefore wise to limit their use to no more than a few microcycles over the course of a periodized program.\" 3 What about cardio? \"A goal of many lifters is to increase muscle development while simultaneously reducing body fat levels. In an attempt to accelerate fat loss, cardio is frequently ramped up while performing intense resistance training. Adding some aerobic training to a muscle-building routine isn\u2019t necessarily a bad thing . Overdoing it, however, certainly is . You see, the signaling pathways for resistance training and aerobic training are contradictory. Some researchers have coined the term \u201cAMPK-PKB switch\u201d whereby aerobic training promotes catabolic processes (AMPK is involved in pathways associated with protein breakdown, which for your sake can be considered \u201cmuscle wasting\u201d) and resistance training promotes anabolic processes (PKB is involved in pathways associated with protein synthesis, or for you, \u201cmuscle gaining\u201d). If your goal is to maximize muscle, keep cardio at moderate levels. How much is too much? It ultimately depends on the individual, as some can tolerate more than others. A general guideline is to limit steady state cardio to no more than about 3 or 4 weekly bouts lasting 30 to 40 minutes .\" 3 Metabolic resistance training (burn fat and gain muscle) \"MRT, a.k.a. \"metabolic resistance training,\" might as well be called \"madman training.\" It's no-holds-barred, haul-ass, maximum-effort, build-muscle, heave-weight, torch-fat, absolutely insane huff-n-puff training. It'll spike your metabolism, crush calories like beer cans, lift your lactate threshold, boost your ability to make muscle, and maximize your body's capacity for change. \" 7 \"MRT works by heightening the metabolic \"cost\" of exercise. This might sound geeky... until you try it. Whereas traditional resistance training might tap 25 or 30% of the body's \"change capacity,\" MRT can maximize your potential for change and unleash metabolic forces that work all day and night. By maximizing your body's change capacity, you can improve 50%\u2014not 25 or 30%\u2014in only 6 weeks . Even better, MRT spreads improvement across multiple desired targets. Basically, when properly integrated into a periodized-training scheme, MRT can help you build muscle, burn fat and gain strength at the same time.\" 7 Basically do whole body workouts really fast with minimal rest between sets and taking sets to muscular failure. Read source 7 for more info.","title":"Science of hypertrophy"},{"location":"BodyBuilding.html#nutrition","text":"General protein intake 1.6-2.2g/kg (varied) protein daily is considered sufficient ; there is no detriment to having more (kidney problems only occur in people with already compromised kidney function) 2 Do not over consume BCAA\u2019s, this may outcompete essential amino acids for absorption 2 BCAA\u2019s are not necessary if overall protein intake is sufficiently varied/complete 2 Anabolic window There is insufficient evidence to strongly conclude that immediate consumption of protein after a workout has significant benefits. As long as there is protein available for synthesis (ie from a meal taken before the workout) there should be no hinderence to hypertrohpy. There is an elevated window of protein synthesis for around 2-4 hours post workout however excessive consumption of protein will not significantly affect hypertrophy. This elevated window remains slightly elevated for 24-36 hours after the initial 2-4 hour spike. 8 ? \"Due to the transient anabolic impact of a protein-rich meal and its potential synergy with the trained state, pre- and post-exercise meals should not be separated by more than approximately 3\u20134 hours, given a typical resistance training bout lasting 45\u201390 minutes.\" 8","title":"Nutrition"},{"location":"BodyBuilding.html#misc","text":"Sources https://en.wikipedia.org/wiki/Muscle_hypertrophy https://www.youtube.com/watch?v=Uc265c-tLGY (Brad Schoenfeld) https://www.bornfitness.com/muscle-building-mistakes/ (Brad Schoenfeld) https://www.t-nation.com/training/4-reasons-youre-not-gaining-muscle (Brad Schoenfeld) http://www.lookgreatnaked.com/articles/does_exercise_induced_muscle_damage_play_a_role_in_skeletal_muscle_hypertrophy.pdf (Brad Schoenfeld) https://www.t-nation.com/training/light-weights-for-big-gains (Brad Schoenfeld) https://www.bodybuilding.com/content/metabolic-resistance-training-build-muscle-and-torch-fat-at-once.html (Brad Schoenfeld) https://jissn.biomedcentral.com/articles/10.1186/1550-2783-10-5 (Brad Shoenfeld)","title":"Misc"},{"location":"ComputerSystems2.html","text":"Computer Systems II \u21e6 University course These notes were made for the University of Southampton COMP2215 module. Terminology \u21e6 Vector table The vector table contains a set of interrupt handlers which are called when there is an interrupt request An interrupt handler contained here is often called a vector (direction the program can go) In AVR-GCC the vector table is predefined meaning it directly sets the program counter to jump to executing interrupt code In AVR-GCC interrupt routines also have a predefined name meaning they will be called automatically as long as they are named appropriately Useful Info \u21e6 Interrupt Vectors Interrupt Vectors \u21e6 A list of every interrupt vector for AT90USB1286 Name Description ADC_vect ADC conversion complete ANALOG_COMP_vect Analog comparator EE_READY_vect EEPROM ready INT0_vect External Interrupt 0 INT1_vect External Interrupt 1 INT2_vect External Interrupt 2 INT3_vect External Interrupt 3 INT4_vect External Interrupt 4 INT5_vect External Interrupt 5 INT6_vect External Interrupt 6 INT7_vect External Interrupt 7 PCINT0_vect Pin Change Interrupt 0 SPI_STC_vect Serial Transfer Complete SPM_READY_vect Store Program Memory Read TIMER0_COMPA_vect Timer Counter 0 Compare Match A TIMER0_COMPB_vect Timer Counter 0 Compare Match B TIMER0_OVF_vect Timer/Counter 0 Overflow TIMER1_CAPT_vect Timer/Counter Capture Event TIMER1_COMPA_vect Timer Counter 1 Compare Match A TIMER1_COMPB_vect Timer Counter 1 Compare Match B TIMER1_COMPC_vect Timer Counter 1 Compare Match C TIMER1_OVF_vect Timer/Counter 1 Overflow TIMER2_COMPA_vect Timer Counter 2 Compare Match A TIMER2_COMPB_vect Timer Counter 2 Compare Match B TIMER2_OVF_vect Timer/Counter 2 Overflow TIMER3_CAPT_vect Timer/Counter Capture Event TIMER3_COMPA_vect Timer Counter 3 Compare Match A TIMER3_COMPB_vect Timer Counter 3 Compare Match B TIMER3_COMPC_vect Timer Counter 3 Compare Match C TIMER3_OVF_vect Timer/Counter 3 Overflow TWI_vect 2-wire Serial Interface USART1_RX_vect USART1 rx complete USART1_TX_vect USART1 tx complete USART1_UDRE_vect USART1 Data register empty WDT_vect Watchdog Timout Interrupt Registers Status I/O Register (SREG) SREG \u21e6 This register has 8 bits containing information about the current status of the microcontroller (specifically about the previous operation performed) C Carry flag Z Zero flag N Negative flag V Two's complement overflow indicator S N xor V for signed tests H Half carry flag T Transfer bit used by BLD and BST Instructions I Global Interrupt Enable/Disable flag Timer/Counter Register (TCNT0) TCNT0 \u21e6 This register gives direct access to the timer/counter unit's 8-bit counter. Modifying this register while the counter is running may cause a Compare Match between TCNT0 and the OCR0x registers to be missed. Output Compare Register A (OCR0A) OCR0A \u21e6 The Output Compare Register A contains an 8-bit value that is continuously compared with the counter value (TCNT0). A match can be used to generate an Output Compare interrupt, or to generate a waveform output on the OC0A pin. Timer/Counter Interrupt Mask Register (TIMSK0) TIMSK0 \u21e6 Enables compare and overflow interrupts for timer 0. Bits 7-3 are reserved and always 0. Bit 2 ( OCIE0B ) Enables the Compare Match B interrupt. Bit 1 ( OCIE0A ) Enables the Compare Match A interrupt. Bit 0 ( TOIE0 ) Enables the overflow interrupt. Even more info? Bit 2 \u2013 OCIE0B Timer/Counter Output Compare Match B Interrupt Enable When the OCIE0B bit is written to one, and the I-bit in the Status Register is set, the Timer/Counter Compare Match B interrupt is enabled. The corresponding interrupt is executed if a Compare Match in Timer/Counter occurs, that is, when the OCF0B bit is set in the Timer/Counter Interrupt Flag Register \u2013 TIFR0. Bit 1 \u2013 OCIE0A Timer/Counter0 Output Compare Match A Interrupt Enable When the OCIE0A bit is written to one, and the I-bit in the Status Register is set, the Timer/Counter0 Compare Match A interrupt is enabled. The corresponding interrupt is executed if a Compare Match in Timer/Counter0 occurs, that is, when the OCF0A bit is set in the Timer/Counter 0 Interrupt Flag Register \u2013 TIFR0. Bit 0 \u2013 TOIE0 Timer/Counter0 Overflow Interrupt Enable When the TOIE0 bit is written to one, and the I-bit in the Status Register is set, the Timer/Counter0 Overflow interrupt is enabled. The corresponding interrupt is executed if an overflow in Timer/Counter0 occurs, that is, when the TOV0 bit is set in the Timer/Counter 0 Interrupt Flag Register \u2013 TIFR0. Timer/Counter Control Register A (TCCR0A) TCCR0A \u21e6 Misc options for the timer/counter Bit 7 COMOA1 Compare Match Output A Mode bit 1 Bit 6 COMOA0 Compare Match Output A Mode bit 0 Bit 5 COMOB1 Compare Match Output B mode bit 1 Bit 4 COMOB0 Compare Match Output B mode bit 0 Bit 3 reserved Bit 2 reserved Bit 1 WGM01 Waveform Generation Mode bit 1 Bit 0 WGM00 Waveform Generation Mode bit 0 COM0X1 COMOX0 Behaviour 0 0 Normal port operation 0 1 Non-PWM Toggle OC0X on Compare Match Fast-PWM WGM02 = 0: Normal Port Operation, OC0X Disconnected. WGM02 = 1: Toggle OC0X on Compare Match. Phase-Correct PWM WGM02 = 0: Normal Port Operation, OC0X Disconnected. WGM02 = 1: Toggle OC0X on Compare Match. 1 0 Non-PWM Clear OC0X on Compare Match Fast-PWM Clear OC0X on Compare Match, set OC0X at TOP Phase-Correct PWM Clear OC0X on Compare Match when up-counting. Set OC0X on Compare Match when down-counting. 1 1 Non-PWM Set OC0X on Compare Match Fast-PWM Set OC0X on Compare Match, clear OC0X at TOP Phase-Correct PWM Set OC0X on Compare Match when up-counting. Clear OC0X on Compare Match when down-counting. WGM2 WGM1 WGM0 Mode of operation TOP Update OCRx at TOV flag set on 0 0 0 Normal OXFF Immediate MAX 0 0 1 PWM phase correct 0xFF TOP BOTTOM 0 1 0 CTC OCRA Immediate MAX 0 1 1 Fast PWM 0xFF TOP 1 0 0 Reserved \u2013 \u2013 \u2013 1 0 1 PWM phase correct OCRA TOP BOTTOM 1 1 0 Reserved \u2013 \u2013 \u2013 1 1 1 Fast PWM OCRA TOP Timer/Counter 0 Interrupt Flag Register (TIFR0) TIFR0 \u21e6 Indicates when the counter triggers a compare or overflow interrupt. Bits 7-3 are reserved and always 0. Bit 2 OCF0B Timer/Counter 0 Output Compare B Match Flag Bit 1 OCF0A Timer/Counter 0 Output Compare A Match Flag Bit 0 TOV0 Timer/Counter 0 Overflow Flag Even more info? Bit 2 \u2013 OCF0B Timer/Counter 0 Output Compare B Match Flag The OCF0B bit is set when a Compare Match occurs between the Timer/Counter and the data in OCR0B \u2013 Output Compare Register0 B. OCF0B is cleared by hardware when executing the corresponding interrupt handling vector. Alternatively,OCF0B is cleared by writing a logic one to the flag. When the I-bit in SREG, OCIE0B (Timer/Counter Compare B Match Interrupt Enable), and OCF0B are set, the Timer/Counter Compare Match Interrupt is executed. Bit 1 \u2013 OCF0A Timer/Counter 0 Output Compare A Match Flag The OCF0A bit is set when a Compare Match occurs between the Timer/Counter 0 and the data in OCR0A \u2013 Output Compare Register 0. OCF0A is cleared by hardware when executing the corresponding interrupt handling vector. Alternatively, OCF0A is cleared by writing a logic one to the flag. When the I-bit in SREG, OCIE0A (Timer/Counter 0 Compare Match Interrupt Enable), and OCF0A are set, the Timer/Counter 0 Compare Match Interrupt is executed. Bit 0 \u2013 TOV0 Timer/Counter 0 Overflow Flag The bit TOV0 is set when an overflow occurs in Timer/Counter 0. TOV0 is cleared by hardware when executing the corresponding interrupt handling vector. Alternatively, TOV0 is cleared by writing a logic one to the flag. When the SREG I-bit, TOIE0 (Timer/Counter 0 Overflow Interrupt Enable), and TOV0 are set, the Timer/Counter 0 Overflow interrupt is executed. The setting of this flag is dependent on the WGM02:0 bit setting. Clock Prescale Register (CLKPR) CLKPR \u21e6 Controls how the system clock prescaler works. Bit 7 CLKPCE Write a 1 to this bit with 0 for all other bits to enable changes. This bit is cleared by hardware. Bits 3-0 CLKPSX Division factor selector bits, see table below CLKPS3 CLKPS2 CLKPS1 CLKPS0 Clock division factor 0 0 0 0 1 0 0 0 1 2 0 0 1 0 4 0 0 1 1 8 0 1 0 0 16 0 1 0 1 32 0 1 1 0 64 0 1 1 1 128 1 0 0 0 256 Odd functions sei() sei() \u21e6 This is used to enable global interrupts. Specifically it sets the I flag in SREG to 1. cli() cli() \u21e6 This is used to disable global interrupts. Specifically it clears the I flag in SREG to 0. Unstructured Notes \u21e6 Why are interrupts so weird? C tries to stay away from machine specific details This means each compiler has to handle interrupts it's own way The most common approach for this is to use a vector table to direct interrupt requests This still has problems though, such as needing to save registers and restore them later To enable saving registers and restoring later we need to tag the interrupt function with __attribute__((signal)) . The ISR() macro does this for us and this is why we use it. Defining an interrupt routine An interrupt routine is defined with ISR() ( I nterrupt S ervice R outine) This is a macro defined by the interrupt api (avr/interrupt.h) Here is an example of the ADC interrupt #include <avr/interrupt.h> ISR ( ADC_vect ) { // user code here } If there is an unexpected interrupt (enabled but no handler) then the default action is to reset the device by jumping to the reset vector. You can catch this with the interrupt routine BADISR_vect . #include <avr/interrupt.h> ISR ( BADISR_vect ) { // user code here } AVR hardware clears the global interrupt flag in SREG before entering an interrupt vector, disabling other interrupts being called during execution. To allow interrupts to be embedded you can either use an sei() instruction at the beginning of the interrupt handler, or you can tell the compiler to do this for you by defining the interrupt handler like this. ISR ( XXX_vect , ISR_NOBLOCK ) { ... } Sometimes interrupts share the exact same code. To allow multiple interrupts to execute this code we can use the ISR_ALIASOF() attribute. ISR ( PCINT0_vect ) { ... // Code to handle the event. } ISR ( PCINT1_vect , ISR_ALIASOF ( PCINT0_vect )); Enabling an interrupt The global interrupt enabled bit must be enabled for any interrupts to execute (see SREG). This is done simply by calling sei() . Each interrupt is associated with 2 bits Interrupt Flag Bit set when the interrupt occurs, regardless of if it is enabled or not, not really important Interrupt Enabled used to enable or disable an interrupt, this is what we want When both the Global Interrupt Enabled bit and the Interrupt Enabled bit are 1 then that interrupt will execute. Helpfully there is no general way to set the interrupt enabled bit and it is different for each one depending on which register needs to be used. Here is a quick cheat sheet on each interrupt. Enable interrupt cheat sheet Interrupt Enable snippet ADC_vect ANALOG_COMP_vect EE_READY_vect INT0_vect INT1_vect INT2_vect INT3_vect INT4_vect INT5_vect INT6_vect INT7_vect PCINT0_vect SPI_STC_vect SPM_READY_vect TIMER0_COMPA_vect TIMSK0 |= _BV(OCIE0A) TIMER0_COMPB_vect TIMER0_OVF_vect TIMER1_CAPT_vect TIMER1_COMPA_vect TIMER1_COMPB_vect TIMER1_COMPC_vect TIMER1_OVF_vect TIMER2_COMPA_vect TIMER2_COMPB_vect TIMER2_OVF_vect TIMER3_CAPT_vect TIMER3_COMPA_vect TIMER3_COMPB_vect TIMER3_COMPC_vect TIMER3_OVF_vect TWI_vect USART1_RX_vect USART1_TX_vect USART1_UDRE_vect WDT_vect How to use timers First of all there are three main things to consider System clock this is the internal clock (8MHz) System Prescaler this divides the system clock to more manageable values and saves power, but this affects everything (CLKPR) Counter Prescaler this also divides the incoming clock to more more manageble values but only affects the counter () The clock increments a counter (TCNTX) and when this counter hits a certain value (OCRXX) a compare match interrupt is executed. We use a prescaler as the timer counter register is tiny! With only 8 bits we can only count 256 pulses, at 8Mhz this is only 0.03 ms!! With a prescaler we count a set number of pulses from the clock before passing a pulse on, extending the max time of the timer at the cost of resolution. The prescaler can divide the frequency by either 8, 64, 256 or 1024. If we used a prescaler with a value of 1024 then the max length of time we can use the timer for is now 32.6 ms. A helpful formula for calculating this is time = count_target / (clock / prescaler) There are four timer modes Normal mode Clear Timer on Compare Match (CTC) mode Fast PWM mode Phase correct PWM mode 16 bit in an 8 bit world? Some registers are 16 bits wide even though our data bus is only 8 bits. An example of this is Timer1 (and 3). The Timer/Counter (TCNTn), Output Compare Registers (OCRnA/B/C), and Input Capture Register (ICRn) are all 16-bit registers. Internally this is handled by a temporary 8 bit register which stores half the value during reads/writes. When writing the high byte must be written first followed by the low byte. When reading the low byte must be read first then the high byte. The CPU handles copying into/from the temporary register for us. ... ; Set TCNTn to 0x01FF ldi r17 , 0x01 ldi r16 , 0xFF out TCNTnH , r17 out TCNTnL , r16 ; Read TCNTn into r17:r16 in r16 , TCNTnL in r17 , TCNTnH ... Luckily when using C this is all abstracted away by the compiler so we can simply read and write like normal. unsigned int i ; ... /* Set TCNTn to 0x01FF */ TCNTn = 0x1FF ; /* Read TCNTn into i */ i = TCNTn ; ... Sources About interrupts https://www.nongnu.org/avr-libc/user-manual/group__avr__interrupts.html About interrupt vector tables https://en.wikipedia.org/wiki/Interrupt_vector_table AVR interrupt vectors https://www.microchip.com/webdoc/AVRLibcReferenceManual/group__avr__interrupts.html More about interrupts http://www.avr-tutorials.com/interrupts/about-avr-8-bit-microcontrollers-interrupts SREG info http://ww1.microchip.com/downloads/en/devicedoc/atmel-0856-avr-instruction-set-manual.pdf Timer info https://sites.google.com/site/qeewiki/books/avr-guide/common-timer-theory","title":"Computer Systems II"},{"location":"ComputerSystems2.html#computer-systems-ii","text":"University course These notes were made for the University of Southampton COMP2215 module.","title":"Computer Systems II"},{"location":"ComputerSystems2.html#terminology","text":"Vector table The vector table contains a set of interrupt handlers which are called when there is an interrupt request An interrupt handler contained here is often called a vector (direction the program can go) In AVR-GCC the vector table is predefined meaning it directly sets the program counter to jump to executing interrupt code In AVR-GCC interrupt routines also have a predefined name meaning they will be called automatically as long as they are named appropriately","title":"Terminology"},{"location":"ComputerSystems2.html#useful-info","text":"Interrupt Vectors","title":"Useful Info"},{"location":"ComputerSystems2.html#interrupt-vectors","text":"A list of every interrupt vector for AT90USB1286 Name Description ADC_vect ADC conversion complete ANALOG_COMP_vect Analog comparator EE_READY_vect EEPROM ready INT0_vect External Interrupt 0 INT1_vect External Interrupt 1 INT2_vect External Interrupt 2 INT3_vect External Interrupt 3 INT4_vect External Interrupt 4 INT5_vect External Interrupt 5 INT6_vect External Interrupt 6 INT7_vect External Interrupt 7 PCINT0_vect Pin Change Interrupt 0 SPI_STC_vect Serial Transfer Complete SPM_READY_vect Store Program Memory Read TIMER0_COMPA_vect Timer Counter 0 Compare Match A TIMER0_COMPB_vect Timer Counter 0 Compare Match B TIMER0_OVF_vect Timer/Counter 0 Overflow TIMER1_CAPT_vect Timer/Counter Capture Event TIMER1_COMPA_vect Timer Counter 1 Compare Match A TIMER1_COMPB_vect Timer Counter 1 Compare Match B TIMER1_COMPC_vect Timer Counter 1 Compare Match C TIMER1_OVF_vect Timer/Counter 1 Overflow TIMER2_COMPA_vect Timer Counter 2 Compare Match A TIMER2_COMPB_vect Timer Counter 2 Compare Match B TIMER2_OVF_vect Timer/Counter 2 Overflow TIMER3_CAPT_vect Timer/Counter Capture Event TIMER3_COMPA_vect Timer Counter 3 Compare Match A TIMER3_COMPB_vect Timer Counter 3 Compare Match B TIMER3_COMPC_vect Timer Counter 3 Compare Match C TIMER3_OVF_vect Timer/Counter 3 Overflow TWI_vect 2-wire Serial Interface USART1_RX_vect USART1 rx complete USART1_TX_vect USART1 tx complete USART1_UDRE_vect USART1 Data register empty WDT_vect Watchdog Timout Interrupt Registers Status I/O Register (SREG)","title":"Interrupt Vectors"},{"location":"ComputerSystems2.html#sreg","text":"This register has 8 bits containing information about the current status of the microcontroller (specifically about the previous operation performed) C Carry flag Z Zero flag N Negative flag V Two's complement overflow indicator S N xor V for signed tests H Half carry flag T Transfer bit used by BLD and BST Instructions I Global Interrupt Enable/Disable flag Timer/Counter Register (TCNT0)","title":"SREG"},{"location":"ComputerSystems2.html#tcnt0","text":"This register gives direct access to the timer/counter unit's 8-bit counter. Modifying this register while the counter is running may cause a Compare Match between TCNT0 and the OCR0x registers to be missed. Output Compare Register A (OCR0A)","title":"TCNT0"},{"location":"ComputerSystems2.html#ocr0a","text":"The Output Compare Register A contains an 8-bit value that is continuously compared with the counter value (TCNT0). A match can be used to generate an Output Compare interrupt, or to generate a waveform output on the OC0A pin. Timer/Counter Interrupt Mask Register (TIMSK0)","title":"OCR0A"},{"location":"ComputerSystems2.html#timsk0","text":"Enables compare and overflow interrupts for timer 0. Bits 7-3 are reserved and always 0. Bit 2 ( OCIE0B ) Enables the Compare Match B interrupt. Bit 1 ( OCIE0A ) Enables the Compare Match A interrupt. Bit 0 ( TOIE0 ) Enables the overflow interrupt. Even more info? Bit 2 \u2013 OCIE0B Timer/Counter Output Compare Match B Interrupt Enable When the OCIE0B bit is written to one, and the I-bit in the Status Register is set, the Timer/Counter Compare Match B interrupt is enabled. The corresponding interrupt is executed if a Compare Match in Timer/Counter occurs, that is, when the OCF0B bit is set in the Timer/Counter Interrupt Flag Register \u2013 TIFR0. Bit 1 \u2013 OCIE0A Timer/Counter0 Output Compare Match A Interrupt Enable When the OCIE0A bit is written to one, and the I-bit in the Status Register is set, the Timer/Counter0 Compare Match A interrupt is enabled. The corresponding interrupt is executed if a Compare Match in Timer/Counter0 occurs, that is, when the OCF0A bit is set in the Timer/Counter 0 Interrupt Flag Register \u2013 TIFR0. Bit 0 \u2013 TOIE0 Timer/Counter0 Overflow Interrupt Enable When the TOIE0 bit is written to one, and the I-bit in the Status Register is set, the Timer/Counter0 Overflow interrupt is enabled. The corresponding interrupt is executed if an overflow in Timer/Counter0 occurs, that is, when the TOV0 bit is set in the Timer/Counter 0 Interrupt Flag Register \u2013 TIFR0. Timer/Counter Control Register A (TCCR0A)","title":"TIMSK0"},{"location":"ComputerSystems2.html#tccr0a","text":"Misc options for the timer/counter Bit 7 COMOA1 Compare Match Output A Mode bit 1 Bit 6 COMOA0 Compare Match Output A Mode bit 0 Bit 5 COMOB1 Compare Match Output B mode bit 1 Bit 4 COMOB0 Compare Match Output B mode bit 0 Bit 3 reserved Bit 2 reserved Bit 1 WGM01 Waveform Generation Mode bit 1 Bit 0 WGM00 Waveform Generation Mode bit 0 COM0X1 COMOX0 Behaviour 0 0 Normal port operation 0 1 Non-PWM Toggle OC0X on Compare Match Fast-PWM WGM02 = 0: Normal Port Operation, OC0X Disconnected. WGM02 = 1: Toggle OC0X on Compare Match. Phase-Correct PWM WGM02 = 0: Normal Port Operation, OC0X Disconnected. WGM02 = 1: Toggle OC0X on Compare Match. 1 0 Non-PWM Clear OC0X on Compare Match Fast-PWM Clear OC0X on Compare Match, set OC0X at TOP Phase-Correct PWM Clear OC0X on Compare Match when up-counting. Set OC0X on Compare Match when down-counting. 1 1 Non-PWM Set OC0X on Compare Match Fast-PWM Set OC0X on Compare Match, clear OC0X at TOP Phase-Correct PWM Set OC0X on Compare Match when up-counting. Clear OC0X on Compare Match when down-counting. WGM2 WGM1 WGM0 Mode of operation TOP Update OCRx at TOV flag set on 0 0 0 Normal OXFF Immediate MAX 0 0 1 PWM phase correct 0xFF TOP BOTTOM 0 1 0 CTC OCRA Immediate MAX 0 1 1 Fast PWM 0xFF TOP 1 0 0 Reserved \u2013 \u2013 \u2013 1 0 1 PWM phase correct OCRA TOP BOTTOM 1 1 0 Reserved \u2013 \u2013 \u2013 1 1 1 Fast PWM OCRA TOP Timer/Counter 0 Interrupt Flag Register (TIFR0)","title":"TCCR0A"},{"location":"ComputerSystems2.html#tifr0","text":"Indicates when the counter triggers a compare or overflow interrupt. Bits 7-3 are reserved and always 0. Bit 2 OCF0B Timer/Counter 0 Output Compare B Match Flag Bit 1 OCF0A Timer/Counter 0 Output Compare A Match Flag Bit 0 TOV0 Timer/Counter 0 Overflow Flag Even more info? Bit 2 \u2013 OCF0B Timer/Counter 0 Output Compare B Match Flag The OCF0B bit is set when a Compare Match occurs between the Timer/Counter and the data in OCR0B \u2013 Output Compare Register0 B. OCF0B is cleared by hardware when executing the corresponding interrupt handling vector. Alternatively,OCF0B is cleared by writing a logic one to the flag. When the I-bit in SREG, OCIE0B (Timer/Counter Compare B Match Interrupt Enable), and OCF0B are set, the Timer/Counter Compare Match Interrupt is executed. Bit 1 \u2013 OCF0A Timer/Counter 0 Output Compare A Match Flag The OCF0A bit is set when a Compare Match occurs between the Timer/Counter 0 and the data in OCR0A \u2013 Output Compare Register 0. OCF0A is cleared by hardware when executing the corresponding interrupt handling vector. Alternatively, OCF0A is cleared by writing a logic one to the flag. When the I-bit in SREG, OCIE0A (Timer/Counter 0 Compare Match Interrupt Enable), and OCF0A are set, the Timer/Counter 0 Compare Match Interrupt is executed. Bit 0 \u2013 TOV0 Timer/Counter 0 Overflow Flag The bit TOV0 is set when an overflow occurs in Timer/Counter 0. TOV0 is cleared by hardware when executing the corresponding interrupt handling vector. Alternatively, TOV0 is cleared by writing a logic one to the flag. When the SREG I-bit, TOIE0 (Timer/Counter 0 Overflow Interrupt Enable), and TOV0 are set, the Timer/Counter 0 Overflow interrupt is executed. The setting of this flag is dependent on the WGM02:0 bit setting. Clock Prescale Register (CLKPR)","title":"TIFR0"},{"location":"ComputerSystems2.html#clkpr","text":"Controls how the system clock prescaler works. Bit 7 CLKPCE Write a 1 to this bit with 0 for all other bits to enable changes. This bit is cleared by hardware. Bits 3-0 CLKPSX Division factor selector bits, see table below CLKPS3 CLKPS2 CLKPS1 CLKPS0 Clock division factor 0 0 0 0 1 0 0 0 1 2 0 0 1 0 4 0 0 1 1 8 0 1 0 0 16 0 1 0 1 32 0 1 1 0 64 0 1 1 1 128 1 0 0 0 256 Odd functions sei()","title":"CLKPR"},{"location":"ComputerSystems2.html#sei","text":"This is used to enable global interrupts. Specifically it sets the I flag in SREG to 1. cli()","title":"sei()"},{"location":"ComputerSystems2.html#cli","text":"This is used to disable global interrupts. Specifically it clears the I flag in SREG to 0.","title":"cli()"},{"location":"ComputerSystems2.html#unstructured-notes","text":"Why are interrupts so weird? C tries to stay away from machine specific details This means each compiler has to handle interrupts it's own way The most common approach for this is to use a vector table to direct interrupt requests This still has problems though, such as needing to save registers and restore them later To enable saving registers and restoring later we need to tag the interrupt function with __attribute__((signal)) . The ISR() macro does this for us and this is why we use it. Defining an interrupt routine An interrupt routine is defined with ISR() ( I nterrupt S ervice R outine) This is a macro defined by the interrupt api (avr/interrupt.h) Here is an example of the ADC interrupt #include <avr/interrupt.h> ISR ( ADC_vect ) { // user code here } If there is an unexpected interrupt (enabled but no handler) then the default action is to reset the device by jumping to the reset vector. You can catch this with the interrupt routine BADISR_vect . #include <avr/interrupt.h> ISR ( BADISR_vect ) { // user code here } AVR hardware clears the global interrupt flag in SREG before entering an interrupt vector, disabling other interrupts being called during execution. To allow interrupts to be embedded you can either use an sei() instruction at the beginning of the interrupt handler, or you can tell the compiler to do this for you by defining the interrupt handler like this. ISR ( XXX_vect , ISR_NOBLOCK ) { ... } Sometimes interrupts share the exact same code. To allow multiple interrupts to execute this code we can use the ISR_ALIASOF() attribute. ISR ( PCINT0_vect ) { ... // Code to handle the event. } ISR ( PCINT1_vect , ISR_ALIASOF ( PCINT0_vect )); Enabling an interrupt The global interrupt enabled bit must be enabled for any interrupts to execute (see SREG). This is done simply by calling sei() . Each interrupt is associated with 2 bits Interrupt Flag Bit set when the interrupt occurs, regardless of if it is enabled or not, not really important Interrupt Enabled used to enable or disable an interrupt, this is what we want When both the Global Interrupt Enabled bit and the Interrupt Enabled bit are 1 then that interrupt will execute. Helpfully there is no general way to set the interrupt enabled bit and it is different for each one depending on which register needs to be used. Here is a quick cheat sheet on each interrupt. Enable interrupt cheat sheet Interrupt Enable snippet ADC_vect ANALOG_COMP_vect EE_READY_vect INT0_vect INT1_vect INT2_vect INT3_vect INT4_vect INT5_vect INT6_vect INT7_vect PCINT0_vect SPI_STC_vect SPM_READY_vect TIMER0_COMPA_vect TIMSK0 |= _BV(OCIE0A) TIMER0_COMPB_vect TIMER0_OVF_vect TIMER1_CAPT_vect TIMER1_COMPA_vect TIMER1_COMPB_vect TIMER1_COMPC_vect TIMER1_OVF_vect TIMER2_COMPA_vect TIMER2_COMPB_vect TIMER2_OVF_vect TIMER3_CAPT_vect TIMER3_COMPA_vect TIMER3_COMPB_vect TIMER3_COMPC_vect TIMER3_OVF_vect TWI_vect USART1_RX_vect USART1_TX_vect USART1_UDRE_vect WDT_vect How to use timers First of all there are three main things to consider System clock this is the internal clock (8MHz) System Prescaler this divides the system clock to more manageable values and saves power, but this affects everything (CLKPR) Counter Prescaler this also divides the incoming clock to more more manageble values but only affects the counter () The clock increments a counter (TCNTX) and when this counter hits a certain value (OCRXX) a compare match interrupt is executed. We use a prescaler as the timer counter register is tiny! With only 8 bits we can only count 256 pulses, at 8Mhz this is only 0.03 ms!! With a prescaler we count a set number of pulses from the clock before passing a pulse on, extending the max time of the timer at the cost of resolution. The prescaler can divide the frequency by either 8, 64, 256 or 1024. If we used a prescaler with a value of 1024 then the max length of time we can use the timer for is now 32.6 ms. A helpful formula for calculating this is time = count_target / (clock / prescaler) There are four timer modes Normal mode Clear Timer on Compare Match (CTC) mode Fast PWM mode Phase correct PWM mode 16 bit in an 8 bit world? Some registers are 16 bits wide even though our data bus is only 8 bits. An example of this is Timer1 (and 3). The Timer/Counter (TCNTn), Output Compare Registers (OCRnA/B/C), and Input Capture Register (ICRn) are all 16-bit registers. Internally this is handled by a temporary 8 bit register which stores half the value during reads/writes. When writing the high byte must be written first followed by the low byte. When reading the low byte must be read first then the high byte. The CPU handles copying into/from the temporary register for us. ... ; Set TCNTn to 0x01FF ldi r17 , 0x01 ldi r16 , 0xFF out TCNTnH , r17 out TCNTnL , r16 ; Read TCNTn into r17:r16 in r16 , TCNTnL in r17 , TCNTnH ... Luckily when using C this is all abstracted away by the compiler so we can simply read and write like normal. unsigned int i ; ... /* Set TCNTn to 0x01FF */ TCNTn = 0x1FF ; /* Read TCNTn into i */ i = TCNTn ; ... Sources About interrupts https://www.nongnu.org/avr-libc/user-manual/group__avr__interrupts.html About interrupt vector tables https://en.wikipedia.org/wiki/Interrupt_vector_table AVR interrupt vectors https://www.microchip.com/webdoc/AVRLibcReferenceManual/group__avr__interrupts.html More about interrupts http://www.avr-tutorials.com/interrupts/about-avr-8-bit-microcontrollers-interrupts SREG info http://ww1.microchip.com/downloads/en/devicedoc/atmel-0856-avr-instruction-set-manual.pdf Timer info https://sites.google.com/site/qeewiki/books/avr-guide/common-timer-theory","title":"Unstructured Notes"},{"location":"DistributedSystemsAndNetworks.html","text":"Distributed Systems & Networks \u21e6 University course These notes were made for the University of Southampton COMP2207 module. Unstructured Notes \u21e6 Link layer The link layer is the lowest layer. It handles linking two networked devices together so other layers don't need to worry what the connection is. It also includes error detection and handling (although higher layers do more of this too). Source General info lecture slides","title":"Distributed Systems & Networks"},{"location":"DistributedSystemsAndNetworks.html#distributed-systems-networks","text":"University course These notes were made for the University of Southampton COMP2207 module.","title":"Distributed Systems &amp; Networks"},{"location":"DistributedSystemsAndNetworks.html#unstructured-notes","text":"Link layer The link layer is the lowest layer. It handles linking two networked devices together so other layers don't need to worry what the connection is. It also includes error detection and handling (although higher layers do more of this too). Source General info lecture slides","title":"Unstructured Notes"},{"location":"IntelligentSystems.html","text":"Intelligent Systems \u21e6 Not widely sourced Content heavily lifted from Matthew Barnes (Southampton Uni CompSci student)! All knowledge presented here is sourced from his public notes. University course These notes were made for the University of Southampton COMP2208 module. Classical AI \u21e6 Blind Search Blind Search \u21e6 An uninformed search generally used when knowledge is restricted. Problem Types \u21e6 Single-state problem Deterministic Observable State is always known The shortest path problem is an example of this. Sensorless / multi-state problem Deterministic Non-Observable Initial state could be anything Guessing a random number. The random number could be anything but it will always be found. Contingency problem Non-Deterministic Partially observable You have to perform an action and observe the reactions to move towards a solution. Exploration problem Unknown state space Don't even know what effect actions have \"explore\" the environment to solve Robot exploration as the robot has no idea where it is, it just reacts to the environment. Single-state Problem Formulation \u21e6 Step Description Example Initial state The state we start in When you start a chess game Action or successor function The transition from one state to another When you move a piece in a chess game Goal test Checks if solution has been found Checkmate is reached Path cost The cost of getting to the solution Moves taken There are multiple path costs that a chess algorithm can adopt. One could be \u2018number of opponent pieces\u2019. This would make the algorithm aggressive , since it would try to reduce the number of opponent pieces as much as it can. Another could be \u2018number of moves taken\u2019. This would make the algorithm more strategic , achieving a checkmate in the least amount of moves. States vs Nodes \u21e6 A state is not a node A state is a representation of a physical configuration A node is a data structure consituting part of a search tree used to find the solution A state is a property of a node Search Strategies \u21e6 Strategies are evaluated with the following factors Completeness does it always find a solution if it exists? Time Complexity how does time taken grow as the solution depth increases Space complexity how does the number of nodes in memory grow Optimality does it always find a least-cost solution? Time and space complexity are measured in terms of b how many children each node reaches (maximum branching factor) d depth of least-cost optimal solution m maximum depth of the state space (may be infinite) Searching using Tree Search \u21e6 To find the solution to a problem you need to find the path from the initial state to a goal state This is most commonly achieved using a type of tree search There are four kinds of tree search algorithms used for this Tree search Complexity Description Breadth-first search Complete yes if branching factor is finite Time O(b d+1 ) Space O(b d+1 ) Optimal Yes if step cost is constant In breadth-first search, all of the nodes at the current depth level are searched before moving onto the next depth level. This is complete (assuming the branching factor isn\u2019t infinite), even with infinite depth or loops . Only problem is, the space complexity is terrible . However, breadth-first search is optimal (finds the least-cost solution) if the step cost is constant . This is true because it finds the shallowest goal node. Depth-first search Complete No, if depth is infinite (or has loops) it'll go on infinitely Time O(b m ) Space O(bm) Optimal No, because deeper solution may be found first In depth-first search, it starts at the root and goes all the way down to the far-left leaf, then backtracks and expands shallower nodes. The space complexity is great (it\u2019s linear) because branches can be released from memory if no solution was found in them . However, it is not complete Depth-limited search Complete No, because the solution may be deeper than the limit Time O(b n ) Space O(bn) Optimal No, because deeper solution might be found first It\u2019s the same as depth-first search , but there\u2019s a depth limit ( n ), and anything below that depth limit doesn\u2019t exist Iterative deepening search Complete Yes Time O(b d ) Space O(bd) Optimal Yes, if step cost is constant This is an applied version of depth-limited search, where we start from depth limit 0 and increment the depth limit until we find our goal . We do this to combine depth-first search\u2019s space efficiency with breadth-first search\u2019s completeness . Although we get those benefits, IDS is a little bit slower than depth-first . Repeated States \u21e6 When searching we may come across an already visited state In a deterministic system this will always move to the same next state Because of this we can recognise already visited states and prevent further exploration This converts our tree searches into graph searches (as we are no longer searching a tree) Bidirectional Search \u21e6 A bidirectional search does two searches One starts at the initial state and the other from the goal state This is quicker as b d/2 + b d/2 is much less than b d At each iteration, the node is checked if it has been discovered by both searches If it has a solution has been found This is only efficient if the predecessor of a node can be easily computed Heuristic Search Heuristic Search \u21e6 History and Philosophy History and Philosophy \u21e6 Local Search Local Search \u21e6 Constraint satisfaction problems Constraint satisfaction problems \u21e6 In a standard search problem the state is viewed as a black box A state can be any data structure that supports A successor function An objective function (fitness) A goal test In constraint satisfaction problem we will define the data structure of the state In particular, a state is defined by variables x_{i} x_{i} with values from the domain D_{i} D_{i} In the goal test we use a set of constraints Graph colouring problem We try to colour regions with three different colours such that adjacent regions have a different colour. For this map the problem is defined as follows Variables are the regions WA, NT, Q, NSW, V, SA, T Domains are the three colours D_{i} = \\{red, green, blue\\} D_{i} = \\{red, green, blue\\} For example WA and NT cannot have the same colour Complete Every variable is assigned a value Consistent No constraint violated This is an example solution for this problem Every region has a colour with no two adjacent regions sharing a colour This solution is therefore complete and consistent This can be generalised as a constraint graph The nodes are the variables and the edges are the constraint No two connected nodes can have the same assignment In addition, the resulting graph is a binary CSP because each constraint relates two variables (each edge connects two nodes) CSPs come in different variants \u21e6 If the CSP has discrete variables And the domain is finite We have n n variables and the domain size is d d Therefore there exists O(d^{n}) O(d^{n}) complete assignments Examples are boolean CSPs, including boolean satisfiability (SAT) which is NP-Complete And the domain is infinite Examples of infinite domains are integers and strings An example for such a CSP is job scheduling where the variables are the start/end days for each job The constraints could be defined like StartJob_{1} + 5 < StartJob_{3} StartJob_{1} + 5 < StartJob_{3} If the CSP has continuous variables An example is start/end times of the Hubble Space Telescope observations The constraints are linear constraints solvable in polynomial time by linear programming Constraints come in different variants too \u21e6 Unary constraints involve a single variable, for example SA \\neq \\neq green Binary constaints involve pairs of variables, for example SA \\neq \\neq WA Higher-order constraints involve 3 or more variables, for example X_{1} + X_{2} = X_{3} X_{1} + X_{2} = X_{3} Some more real-world CSPs are Assignment problems, e.g who teaches what class Timetabling problems, e.g which class is offered when and where Transportation scheduling Factory scheduling Note how many real-world problems involve real-valued (continuous) variables! Missing developing an algorithm for CSPs Adversarial Games: Minimax Adversarial Games: Minimax \u21e6 Alpha-beta Pruning Alpha-beta Pruning \u21e6 Alpha-beta pruning goes even beyond minimax We have strong and weak methods to improve minimax Strong methods take into account the game itself , ie board symmetry in tic tac toe Weak methods can apply to any game , one of which is alpha-beta pruning What is pruning? A good example of pruning is the symmetry in tic tac toe A part of the full game tree looks like this Some of these states are redudant, we can achieve them just by rotating the board By removing the redundant states we can prune the search tree a lot Not every game can be pruned in this way! How does alpha-beta pruning work? Minimax explores the entire tree to a given ply depth It evaluates the leaves It propagates the values of these leaves back up the tree Alpha-beta pruning performs DFS but allows us to prune/disregard certain branches of the tree Alpha represents the lower bound on the node value. It is the worst we can do . It is associated with MAX nodes and since it is the worst it never decreases. Beta represents the upper bound on the node value. It is the best we can do . It is associated with MIN nodes and since it is the best it never increases. If the best we can do on the current branch is less than or equal to the worst we can do elsewhere, there is no point continuing this branch! This is very similar to minimax except we can return early if the alpha/beta values indicate to do so Alpha-beta is guarenteed to give the same values as minimax except we reduce the search space. If the tree is ordered , the time complexity is O(b d/2 ) (minimax is O(b d )!) Perfect ordering is not possible as we wouldn't need alpha-beta in the first place! Planning Planning \u21e6 A plan is a sequence of actions to perform tasks and achieve objectives This means generating and searching over possible plans The classical planning environment is fully oberservable, deterministic, finite, static and discrete. It helps to assist humans in practical applications such as Design and manufacuring Military operations Games Space exploration In the real world we have a huge planning environment. We need need to reduce this environment by using heuristics and decomposing the problem. What is a planning language? In order to define plans we need a planning language A good planning language should Be expressive enough to describe a wide variety of problems Be restrictive enough to allow efficient algorithms to operate on it Be able to take advantage of the logical structure of the problem Examples of planning languages are The STRIPS (Stanford Research Institute Problem Solver) model ADL (Action Description Language) Typically planning languages involve States decompose the world into logical conditions and represent a state as a collection of these conditions, for example OnTable(Box1) \u22c0 OnFloor(Box2) Goals a goal is a partially specified state, if any state contains all the literals (conditions) of the goal then this state is considered a goal state Actions a precondition and an effect, we can only perform the effect if the precondition is already met, for example we can only put a box on a table if it isn't already on the table In an action there needs to exist a substitution for all variables in the precondition! Air cargo transport Part Definition Initial State Init( At(C1, SFO) \u22c0 At(C2, JFK) \u22c0 At(P1, SFO) \u22c0 At(P2, JFK) \u22c0 Cargo(C1) \u22c0 Cargo(C2) \u22c0 Plane(P1) \u22c0 Plane(P2) \u22c0 Airport(JFK) \u22c0 Airport(SFO) ) Goal State Goal( At(C1, JFK) \u22c0 At(C2, SFO) ) Actions Action( Load(c, p, a) PRECOND : At(c, a) \u22c0 At(p, a) \u22c0 Cargo(c) \u22c0 Plane(p) \u22c0 Airport(a) EFFECT : \u00acAt(c, a) \u22c0 In(c, p) ) Example plan [Load(C1, P1, SFO), Fly(P1, SFO, JFK), Load(C2, P2, JFK), Fly(P2, JFK, SFO)] Spare tire problem Part Definition Initial State Init(At(Flat, Axle) \u22c0 At(Spare, trunk)) Goal State Goal(At(Spare, Axle)) Actions Action( Remove(Spare, Trunk) PRECOND : At(Spare, Trunk) EFFECT : \u00acAt(Spare, Trunk) \u22c0 At(Spare, Ground) ) Action( Remove(Flat, Axle) PRECOND : At(Flat, Axle) EFFECT : \u00acAt(Flat, Axle) \u22c0 At(Flat, Ground) ) Action(PutOn(Spare, Axle) PRECOND : At(Spare, Ground) \u22c0 \u00acAt(Flat, Axle) `EFFECT : At(Spare, Axle) \u22c0 \u00acAt(Spare, Ground) ) Action( LeaveOvernight EFFECT : \u00acAt(Spare, Ground) \u22c0 \u00acAt(Spare, Axle) \u22c0 \u00acAt(Spare, trunk) \u22c0 \u00acAt(Flat, Ground) \u22c0 \u00acAt(Flat, Axle) ) Example plan [Remove(Flat, Axle), Remove(Spare, Trunk), PutOn(Spare, Axle)] There are two main approaches to making an algorithm which can come up with a plan Forward search, progression planners do forward state-space search. They consider the effect of all possible actions in a given state and search from the initial state to the goal state . Backward search, regression planners do backward state-space search. They consider what must have been true in the previous state to a goal state and therefore search from the goal state to the initial state . Forward searches are nothing more than any complete graph search algorithm, for example A*. Backward searches find actions which have effects that are currently satisfied and pre-conditions which are preferred. These actions must not undo desired literals . This prevents unnecessary actions being considered and therefore reduces the branching factor. Both progression and regression perform poorly without a good heurisic To find an admissile heuristic there are generally two approaches Use the optimal solution to the relaxed problem, e.g. where we remove all preconditions from actions Use the sub-goal independence assumption: The cost of solving a conjunction of subgoals is approximated by the sum of the costs of solving the sub-problems independently. Partial-order Planning \u21e6 Progression and regression planning are forms of totally ordered plan searches We can improve this by specifying some actions which can be done in parallel For these actions it doesn't matter which comes first Each plan has 4 components A set of actions (steps of the plan) A set of ordering constraints A < B A < B (A has to come before B) A set of causal links A\\xrightarrow{p}B A\\xrightarrow{p}B or A\\xrightarrow{} p\\xrightarrow{} B A\\xrightarrow{} p\\xrightarrow{} B (A achieves p for B) A set of open preconditions Shoe example Part Definition Initial State Init( ) Goal State Goal( RightShoeOn \u22c0 LeftShoeOn ) Actions Action( RightShoe PRECOND : RightSockOn EFFECT : RightShoeOn ) Action( RightSock PRECOND : EFFECT : RightSockOn ) Action( LeftShoe PRECOND : LeftSockOn EFFECT : LeftShoeOn ) Action( LeftSock PRECOND : EFFECT : LeftSockOn ) Actions = {Rightsock\\\\ Rightshoe\\\\ Leftsock\\\\ Leftshoe\\\\ Start\\\\ Finish} Actions = {Rightsock\\\\ Rightshoe\\\\ Leftsock\\\\ Leftshoe\\\\ Start\\\\ Finish} Orderings = {Rightsock < Rightshoe \\\\ Leftsock < Leftshoe} Orderings = {Rightsock < Rightshoe \\\\ Leftsock < Leftshoe} Links = {Rightsock \\xrightarrow{Rightsockon} Rightshoe \\\\ Leftsock \\xrightarrow{Leftsockon} Leftshoe \\\\ Rightshoe \\xrightarrow{Rightshoeon} Finish \\\\ Leftshoe \\xrightarrow{Leftshowon} Finish} Links = {Rightsock \\xrightarrow{Rightsockon} Rightshoe \\\\ Leftsock \\xrightarrow{Leftsockon} Leftshoe \\\\ Rightshoe \\xrightarrow{Rightshoeon} Finish \\\\ Leftshoe \\xrightarrow{Leftshowon} Finish} We can put either shoe on first it doesn't matter. This lets us create a partially-ordered plan When is a plan final? A consistent plan with no open preconditions is a solution. A plan is consistent if and only if there are no cycles in the ordering constraints and no conflicts with the links. A POP is executed by repeatedly choosing any of the possible next actions. Missing resolving POP Biologically/Statistically Inspired AI \u21e6 Classification Classification \u21e6 Reasoning Reasoning \u21e6 Reasoning deals with updating our \"belief model\" There are different approaches to reasoning Logic/Rule based we build up basic rules (axioms) using logic which we can derive other rules from Stochastic reasoning reasoning based on probabilities Total Probability The total probabilty of an event B is defined as P(B) = \\sum_{a} P(B\\, and\\, (A=a)) = \\sum_{a} P(B|A = a)P(A=a) P(B) = \\sum_{a} P(B\\, and\\, (A=a)) = \\sum_{a} P(B|A = a)P(A=a) The total probability for event B is the sum of the probabilities for event B and all cases of event A Conditional Probability The conditional probability of an event B is defined as P(B|A) = \\frac{P(A\\, and\\, B)}{P(A)} P(B|A) = \\frac{P(A\\, and\\, B)}{P(A)} P(A|B)P(B) = P(A\\, and\\, B) = P(B|A)P(A) P(A|B)P(B) = P(A\\, and\\, B) = P(B|A)P(A) P(A|B) P(A|B) means \"the probability of event A given that B is true\" Why is it defined like this? Let's look at an example to explain this. We have a bag with red and blue marbles, and we want to know the probabilities of drawing a red/blue marble. We can visualise the probabilities for 2 consecutive draws like this What is the probability of drawing a blue marble, and then another blue one? It's a chance of \\frac{2}{5} \\frac{2}{5} followed by a chance of \\frac{1}{4} \\frac{1}{4} so overall it's \\frac{1}{10} \\frac{1}{10} Now define event A to be \"draw a blue marble first\" and define event B to be \"draw a blue marble second\" We have just calculated P(A\\, and\\, B) = P(A) * P(B|A) P(A\\, and\\, B) = P(A) * P(B|A) Bayes' Rule Bayes' rule is defined as P(B|A) = \\frac{P(A|B)P(B)}{P(A)} P(B|A) = \\frac{P(A|B)P(B)}{P(A)} We can derive this rule with the forumlas in conditional probability. Informally this gives us the probability for B given that A is true. We can think of Bayes' rule as P(hypothesis|evidence) = \\frac{P(hypothesis)P(evidence|hypothesis)}{P(evidence)} P(hypothesis|evidence) = \\frac{P(hypothesis)P(evidence|hypothesis)}{P(evidence)} We can caclulate the probability of our hypothesis based on some evidence which are events we have observed already Monty hall problem There are three doors, two hides goats the other hides a car We choose door 1 The host opens door 2 and there is a goat We can now stay with door 1 or swap to door 3 P(win) P(win) if we stay with door 1 is \\frac{1}{3} \\frac{1}{3} as we made our choice when there were 3 doors What if we swap to door 3? P(B) = \\sum_{a} P(B|A = a)P(A=a) P(B) = \\sum_{a} P(B|A = a)P(A=a) (see total probability) Let B = win and A = car behind door 1 (lose) P(win) = P(win | door1) * P(door1) + P(win | \u00acdoor1) * P(\u00acdoor1) P(win) = P(win | door1) * P(door1) + P(win | \u00acdoor1) * P(\u00acdoor1) P(win | door1) = 0 P(win | door1) = 0 we switched to door 3! P(door1) = \\frac{1}{3} P(door1) = \\frac{1}{3} 1 car 2 goats so 1 in 3 P(win | \u00acdoor1) = 1 P(win | \u00acdoor1) = 1 if not behind door1 it must be behind door3 P(\u00acdoor1) = \\frac{2}{3} P(\u00acdoor1) = \\frac{2}{3} as door2 has been opened by the host P(win) = 0 * \\frac{1}{3} + 1 * \\frac{2}{3} = \\frac{2}{3} P(win) = 0 * \\frac{1}{3} + 1 * \\frac{2}{3} = \\frac{2}{3} Independence Two random variables are independent if their joint probability is the product of their probabilities P(A\\, and\\, B) = P(A) * P(B) P(A\\, and\\, B) = P(A) * P(B) Therefore the conditional probabilities of A and B are equal to the probability of the event itself! P(A|B) = \\frac{P(A\\, and\\, B)}{P(B)} = \\frac{P(A) * P(B)}{P(B)} = P(A) P(A|B) = \\frac{P(A\\, and\\, B)}{P(B)} = \\frac{P(A) * P(B)}{P(B)} = P(A) P(B|A) = P(B) P(B|A) = P(B) Bayesian Belief Update \u21e6 In bayesian reasoning we always keep a belief model when we make our decisions We use probabilities to capture uncertainty in our knowledge We also need to update our belief model after we have seen some more evidence How do we update our belief after each observation? P(model|observation) = \\frac{P(model)P(observation|model)}{P(observation)} P(model|observation) = \\frac{P(model)P(observation|model)}{P(observation)} We know all the probabilities on the right side P(model) P(model) is called the prior . It is the probability that the model is true without taking any observation into account. P(observation|model) P(observation|model) is called the likelihood . It is the probability that the observed event occurs given that the model is true. P(observation) P(observation) is the probabilty that the observation occurs in general. We want to calculate the left hand side P(model|observation) P(model|observation) is call the posterior . It is the probability that the model is true given that we have seen the observation. Complex Knowledge Representation \u21e6 So far we only considered a simple correlation between probabilities but what if we have a much more complicated network of correlations? How can we apply inference in complex networks (ie how can we apply Baye's rule) there? Joint Distribution The simplest way to do inference is to look at the joint distribution of the probability values Here is an example from an employment survey Male Long hours Rich Probability T T T 0.13 T T F 0.11 T F T 0.10 T F F 0.34 F T T 0.01 F T F 0.04 F F T 0.02 F F F 0.25 Now we can ask questions like \"What is the probability that the person is rich?\" We need to calculate the total probability of P(Rich) P(Rich) As we have the table we can sum up all probabilities where Rich = True Rich = True P(Rich) = 0.13 + 0.10 + 0.01 + 0.02 = 0.26 P(Rich) = 0.13 + 0.10 + 0.01 + 0.02 = 0.26 What about the probability a person works for long hours given that he is a male? We need to calculate P(Longhours | Male) P(Longhours | Male) This is equal to \\frac{P(Longhours\\, and\\, Male)}{P(Male)} \\frac{P(Longhours\\, and\\, Male)}{P(Male)} P(Longhours\\, and\\, Male) = 0.13 + 0.11 = 0.24 P(Longhours\\, and\\, Male) = 0.13 + 0.11 = 0.24 P(Male) = 0.13 + 0.11 + 0.10 + 0.34 = 0.68 P(Male) = 0.13 + 0.11 + 0.10 + 0.34 = 0.68 P(Longhours | Male) = 0.24 / 0.68 = 0.35 P(Longhours | Male) = 0.24 / 0.68 = 0.35 We can do any inference from joint distribution however this does not scale well in practice . For example if we had 30 variables we would need a table with 2 30 entries ! This also does not account for independant variables which we can discard completely! Bayesian Networks We can graphically represent the network like this S = studied for the exam M = lecturer is in a good mood H = high marks Every circle (node) is a random variable Every directed edge from node A to B means B depends on A Therefore H depends on both S and M There is no directed edge between S and M, they are independent What is we wanted to calculate \"Given high marks, what is the probability that the lecturer was in a good mood?\" This is P(M|H) = \\,? P(M|H) = \\,? We will use Bayes' rule P(M|H) = \\frac{P(M)P(H|M)}{P(H)} P(M|H) = \\frac{P(M)P(H|M)}{P(H)} We know that P(M) = 0.3 P(M) = 0.3 but we don't know P(H) P(H) or P(H|M) P(H|M) yet P(H|M)\\, = P(H | M\\, and\\, S) * P(S) + P(H | M\\, and\\, \u00acS) * P(\u00acS)\\\\\\qquad\\qquad = 0.9 * 0.8 + 0.5 * 0.2\\\\\\qquad\\qquad = 0.82 P(H|M)\\, = P(H | M\\, and\\, S) * P(S) + P(H | M\\, and\\, \u00acS) * P(\u00acS)\\\\\\qquad\\qquad = 0.9 * 0.8 + 0.5 * 0.2\\\\\\qquad\\qquad = 0.82 P(H) = P(H| M\\, and\\, S) * P(M\\, and\\, S)\\\\\\qquad\\qquad + P(H | M\\, and\\, \u00acS) * P(M\\, and\\, \u00acS)\\\\\\qquad\\qquad + P(H | \u00acM\\, and\\, S) * P(\u00acM\\, and\\, S)\\\\\\qquad\\qquad + P(H | \u00acM\\, and\\, \u00acS) * P(\u00acM\\, and\\, \u00acS) P(H) = P(H| M\\, and\\, S) * P(M\\, and\\, S)\\\\\\qquad\\qquad + P(H | M\\, and\\, \u00acS) * P(M\\, and\\, \u00acS)\\\\\\qquad\\qquad + P(H | \u00acM\\, and\\, S) * P(\u00acM\\, and\\, S)\\\\\\qquad\\qquad + P(H | \u00acM\\, and\\, \u00acS) * P(\u00acM\\, and\\, \u00acS) As we know M and Sare independant we know their joint probability is just their product P(H) = 0.9 * 0.3 * 0.8 \\\\\\qquad\\qquad + 0.5 * 0.3 * 0.2 \\\\\\qquad\\qquad + 0.4 * 0.7 * 0.8 \\\\\\qquad\\qquad + 0.05 * 0.7 * 0.2 \\\\\\quad\\quad\\;\\; = 0.216 + 0.03 + 0.224 + 0.007 \\\\\\quad\\quad\\;\\; = 0.477 P(H) = 0.9 * 0.3 * 0.8 \\\\\\qquad\\qquad + 0.5 * 0.3 * 0.2 \\\\\\qquad\\qquad + 0.4 * 0.7 * 0.8 \\\\\\qquad\\qquad + 0.05 * 0.7 * 0.2 \\\\\\quad\\quad\\;\\; = 0.216 + 0.03 + 0.224 + 0.007 \\\\\\quad\\quad\\;\\; = 0.477 Now we can finally calculate P(M|H) P(M|H) ! P(M|H) = \\frac{P(M)P(H|M)}{P(H)} = \\frac{0.3 * 0.82}{0.477} = 0.516 P(M|H) = \\frac{P(M)P(H|M)}{P(H)} = \\frac{0.3 * 0.82}{0.477} = 0.516 The probability the lecturer was in a good mood, given high marks, is about 52% Properties of Bayesian Networks Must be directed acyclic graphs Major efficieny is that we have economized on memory Easier for humans to interpret Why is it good to use Bayesian Networks? It captures uncertainty of our knowledge about the environ in a very elegant and simple way We can integrate our prior knowledge into the reasoning by using the prior distribution (which represents our prior knowledge) We can always update our belief about the world by using Baye's Theorem Why is it bad to use Bayesian Networks? If we use a wrong prior then it will be difficult to get correct answers The calculation includes integrals and summing over all possible situations (which is typically computationally very expensive!) Descision Making Descision Making \u21e6 So far we have looked at classification and updating our belief model. Now we will look into actual decision making and updating our decision making policy. We will do this by using 2 simple models. Multi-armed bandits (bandit theory) Bandit Theory \u21e6 Suppose we want to outsouce a robbery. There are three different guys from which we can choose one, but we don't know how effective they are. The expected reward per robbery is 50 for the first guy, -10 for the second guy and 30 for the third guy. But remember we don't know that . The expected reward is an average value, meaning that they won't always achieve exactly this value. Therefore our goal is to find out that the first guy is most effective via repeated robbing in multiple rounds. Each round we choose exactly one guy. Who should we hire each round? It is called exploration when we just try out different guys and record their performance guy 1 guy 2 guy 3 50 -10 30 27 -3 41 69 4 22 55 -17 31 ... ... ... 72 -6 33 ----- average ------ 51.3 -8.6 29.5 We will eventually get to their expected values earlier however there is a limit to the number of rounds! We also want to find the best guy as soon as possible so that we can maximise our total reward! It is called exploitation when we always choose the one which we think is best. There is a trade-off between exploration and exploitation. If we focus too much on exploration We can accurately estimate each guy's expected performance But it will take too much time , and we'll end up with a low total reward If we focus too much on exploitation We can focus on maximising the total reward But we might miss a bandit with a higher reward Multi-armed Bandit (MAB) Model \u21e6 The key challenge is to efficiently balance exploration and exploitation such that we maximise the total reward. There are multiple arms with different distributions (probabilities that we win). We don't know the distribution of the arms. Here is an example model with three arms. We are going to \"play\" with the arms for multiple rounds, where we can select one in each round. Our objective is to maximise the total reward. How explorative or exploitative should we be? The Multi-armed Bandit Model is the simplest model that captures the dilemma of exploration vs exploitation. How can we solve the MAB problem? We dont have knowledge about the expected reward values What is the optimal total value? Can we achieve the optimal total value? Our goal is to design algorithms that are close to the optimum as much as possible (good approximation). There are two different approaches to solving this. Epsilon-first approach Suppose the number of rounds T is fixed Choose an epsilon value 0 < epsilon < 1 (typically between 0.05 and 0.2) In the first epsilon*T rounds we only do exploration by pulling the arms in a round robin manner After that we do exploitation by choosing the arm with the highest average reward value Suppose epsilon is 0.1 and T is 100 In the first 10 rounds (0.1 * 100) we pull all three arms in a round robin manner After the first 10 rounds we calculate the arm with the highest average reward In the last 90 rounds (0.9*100) we pull this arm only Epsilon-greedy appraoch Choose an epsilon value 0 < epsilon < 1 (typically between 0.05 and 0.2) Pull the arm with the current best average reward value with probability 1-epsilon Pull one of the other arms uniformly at random Repeat this for each round Suppose epsilon is 0.1 We pull the arm with the current best average reward 90% of the time We pull a random arm 10% of the time Epsilon-first Epsilon-greedy \u2714\ufe0fTypically very good when T is small \u2714\ufe0fTypically efficient when T is sufficiently large \u2757Needs to know T in advance \u2757Slow convergance at start \u2757Sensitive to epsilon value \u2757Sensitive to epsilon value Other Algorithms \u21e6 There are also other algorithms, not just epsilon-first and epsilon-greedy. One example is Upper Confidence Bound (UCB) which combines explorations and exploitation within each single round in a very clever way. Another example is Thompson-sampling which involves maintaining a belief distribution about the true expect reward of each arm using Bayes' Theorem. Randomly sample from each of these believes then choose the arm with the highest sample. We repeat this each round. Performance of Bandit Algorithms and Regret How can we compare the different algorithms? How can we measure the goodness of an algorithm in general? Can we always achieve the optimal solution (best possible)? Spoilers We cannot achieve the best possible! Our aim is to design algorithms that have close performance to that of the best possible. One measurement to quantify the performance is called regret. Regret is the difference between the performance of an algorithm with that of the best possible for a certain number of steps taken . Let's go back to the robbery example We have 3 arms and we pull 100 times Arm 1, 2 and 3 have the expected rewards 50, -10 and 30 Remember we aren't supposed to know that! The best possible algorithm would choose arm 1 all 100 times, therefore the total reward would be 5000 Suppose our algorithm chooses arm 2 20 times and arm 3 10 times and for the rest it pulls arm 1. The total reward is -10*20 + 30*10 + 50*70 = 3600 The difference (ie regret ) is 5000 - 3600 = 1400 We can then say the regret for this algorithm in 100 time steps is 1400 Does there exist a no-regret algorithm? Hold-on! A no-regret algorithm is an algorithm where its average regret converages to 0 when the number of time steps approaches infinity . This indicates that, on average, the algorithm always pulls the best arm. This also indicates that, on average, a no-regret algorithm will start to behave like the optimal (best) algorithm. That's exactly what we want! Extensions of the Multi-armed Bandit There are some extensions to the multi-armed bandit model. budget-limited bandit we have to pay a cost to pull an arm with a total budget limit dueling bandits we choose 2 bandits at each round and we only check which one is better but not the actual reward values best-arm bandit we aim to identify the best arm, we do this by pure learning (only exploration no exploitation) Markov decision process (reinforcement learning) Reinforcement Learning \u21e6 How do humans learn? They learn from experiences . Let's start with an example problem, an agent wants to find the way out from a maze. At each time step the agent Chooses a direction Makes a step Checks whether it's the exit This is a standard search problem! Let's set a new goal for our problem. The agent wants to find the shortest path from A (entrance) to B (exit). What we want to have is a policy of behaviour, at each situation it will tell the agent what to do. Feedback In reinforcement learning, the agent receives feedback on how good the chosen action was There is continuous feedback until the goal is reached It is inbetween supervised and unsupervised learning The agent repeatedly interacts with the environment It gets some feedback, positive or negative , hence reinforcement It is a learning problem because we try to find a good policy based on the feedback A policy is a set of rules that tells the agent what to do at each state How do we know what actions lead us to victory (our goal)? What about those that made us lose the game? How can we measure which action is the best to take at each time step? We need to be able to evaluate the actions we take and the states we are in States, Actions and Rewards \u21e6 Let's define states, actions and rewards We can think about the world as a set of states, there are good and bad states (less or more ideal) With an action we move from one state to another The reward is the feedback of the environment, it measures how good an action was We want to maximise the sum of collected rewards over time. Temporal Difference Learning Suppose we try to find the shortest path for some maze. There are six states where one is the terminal state (exit state). We will start at state A and try to reach state F, which gives a reward of 100 All other states, initially, have a reward of 0 We want to maximise the rewards over time, which is equivalent to finding the shortest path At the beginning we don't have any prior knowledge so we have to start with a really simple policy , just randomly move at each state ! Later when they have reward values we will always move to the state with the highest value Sometime later we eventually arrive at F where we receive a reward of 100 What was the last state before F? Surely that state must be pretty good too We then update the value of that state to be good If we repeat this process we are performing temporal difference learning We maintain the value V of each state, they represent how good a state is How are we going to update our esitmate of each state's value? Immediate reward is important, if moving into a certain state gives us a reward or punishment we need to record that straight away! But future reward is important too. A state may give us nothing now but we still like it if it is linked to future reinforcement It is also important not to let any single learning experience change our opinion too much We want to change our V estimates gradually to allow the long-run picture to emerge We need a learning rate The formula for temporal difference learning combines these factors Current reward Future reward Learning rate Suppose we go from state i to state j then we will update the value of state i What is the learning rate? It determines to what extent newly acquired information overrides old information If we choose a=0 then the agent learns nothing If we choose a=1 then the agen only considers the most recent information Ideally we choose some value between 0 and 1 But wait, there is no negative feedback! Eventually all states will reach a max good value! We need some way to distinguish paths that require less moves. To do this we use a discount factor with a value between 0 and 1. Q-Learning In some cases we also need to learn the outcomes of the actions. This is the case if we don't know which actions will take us to which state. In other words we want to learn the quality of taking each action at each state. This is called the Q value for a specific action and a specific state. The Q value for state i and action k which leads to state j* with actions x** is calculated by Markov Decision Processes (MDP) and Markov Chain \u21e6 In real world problems the state transitions are often stochastic (random) Here is an example You as a student want to graduate within 4 years Obviously there are actions you can take to achieve this goal However there will be some random chance which plays some role too We can model the probabilities of taking a certain action in a Markov chain Suppose the current state is state 3 If we take the action there is a 50% probability that we go to state 1 , and a 50% probability we go to state 4 If the probability of arriving at the next state only depends on the current state and action (not any previous states) then the process has a Markov property We can also model the probabilities in an n by n matrix P P with P_{ij} P_{ij} being the probability of making a transition from state i to state j After we have define the matrix we can also calculate the probability of being in each state after k steps In our example we take state 1 as start state and model this vector b = (1, 0, 0, 0) b = (1, 0, 0, 0) Now to caculate the probabilities of being in each state after k steps we will simply calculate b^{t}P^{k} b^{t}P^{k} We will get a new column vector with the respective probabilities in each state We can also apply temporal difference and Q-Learning to MDP but we need to add the probability for the state transition","title":"Intelligent Systems"},{"location":"IntelligentSystems.html#intelligent-systems","text":"Not widely sourced Content heavily lifted from Matthew Barnes (Southampton Uni CompSci student)! All knowledge presented here is sourced from his public notes. University course These notes were made for the University of Southampton COMP2208 module.","title":"Intelligent Systems"},{"location":"IntelligentSystems.html#classical-ai","text":"Blind Search","title":"Classical AI"},{"location":"IntelligentSystems.html#blind-search","text":"An uninformed search generally used when knowledge is restricted.","title":"Blind Search"},{"location":"IntelligentSystems.html#problem-types","text":"Single-state problem Deterministic Observable State is always known The shortest path problem is an example of this. Sensorless / multi-state problem Deterministic Non-Observable Initial state could be anything Guessing a random number. The random number could be anything but it will always be found. Contingency problem Non-Deterministic Partially observable You have to perform an action and observe the reactions to move towards a solution. Exploration problem Unknown state space Don't even know what effect actions have \"explore\" the environment to solve Robot exploration as the robot has no idea where it is, it just reacts to the environment.","title":"Problem Types"},{"location":"IntelligentSystems.html#single-state-problem-formulation","text":"Step Description Example Initial state The state we start in When you start a chess game Action or successor function The transition from one state to another When you move a piece in a chess game Goal test Checks if solution has been found Checkmate is reached Path cost The cost of getting to the solution Moves taken There are multiple path costs that a chess algorithm can adopt. One could be \u2018number of opponent pieces\u2019. This would make the algorithm aggressive , since it would try to reduce the number of opponent pieces as much as it can. Another could be \u2018number of moves taken\u2019. This would make the algorithm more strategic , achieving a checkmate in the least amount of moves.","title":"Single-state Problem Formulation"},{"location":"IntelligentSystems.html#states-vs-nodes","text":"A state is not a node A state is a representation of a physical configuration A node is a data structure consituting part of a search tree used to find the solution A state is a property of a node","title":"States vs Nodes"},{"location":"IntelligentSystems.html#search-strategies","text":"Strategies are evaluated with the following factors Completeness does it always find a solution if it exists? Time Complexity how does time taken grow as the solution depth increases Space complexity how does the number of nodes in memory grow Optimality does it always find a least-cost solution? Time and space complexity are measured in terms of b how many children each node reaches (maximum branching factor) d depth of least-cost optimal solution m maximum depth of the state space (may be infinite)","title":"Search Strategies"},{"location":"IntelligentSystems.html#searching-using-tree-search","text":"To find the solution to a problem you need to find the path from the initial state to a goal state This is most commonly achieved using a type of tree search There are four kinds of tree search algorithms used for this Tree search Complexity Description Breadth-first search Complete yes if branching factor is finite Time O(b d+1 ) Space O(b d+1 ) Optimal Yes if step cost is constant In breadth-first search, all of the nodes at the current depth level are searched before moving onto the next depth level. This is complete (assuming the branching factor isn\u2019t infinite), even with infinite depth or loops . Only problem is, the space complexity is terrible . However, breadth-first search is optimal (finds the least-cost solution) if the step cost is constant . This is true because it finds the shallowest goal node. Depth-first search Complete No, if depth is infinite (or has loops) it'll go on infinitely Time O(b m ) Space O(bm) Optimal No, because deeper solution may be found first In depth-first search, it starts at the root and goes all the way down to the far-left leaf, then backtracks and expands shallower nodes. The space complexity is great (it\u2019s linear) because branches can be released from memory if no solution was found in them . However, it is not complete Depth-limited search Complete No, because the solution may be deeper than the limit Time O(b n ) Space O(bn) Optimal No, because deeper solution might be found first It\u2019s the same as depth-first search , but there\u2019s a depth limit ( n ), and anything below that depth limit doesn\u2019t exist Iterative deepening search Complete Yes Time O(b d ) Space O(bd) Optimal Yes, if step cost is constant This is an applied version of depth-limited search, where we start from depth limit 0 and increment the depth limit until we find our goal . We do this to combine depth-first search\u2019s space efficiency with breadth-first search\u2019s completeness . Although we get those benefits, IDS is a little bit slower than depth-first .","title":"Searching using Tree Search"},{"location":"IntelligentSystems.html#repeated-states","text":"When searching we may come across an already visited state In a deterministic system this will always move to the same next state Because of this we can recognise already visited states and prevent further exploration This converts our tree searches into graph searches (as we are no longer searching a tree)","title":"Repeated States"},{"location":"IntelligentSystems.html#bidirectional-search","text":"A bidirectional search does two searches One starts at the initial state and the other from the goal state This is quicker as b d/2 + b d/2 is much less than b d At each iteration, the node is checked if it has been discovered by both searches If it has a solution has been found This is only efficient if the predecessor of a node can be easily computed Heuristic Search","title":"Bidirectional Search"},{"location":"IntelligentSystems.html#heuristic-search","text":"History and Philosophy","title":"Heuristic Search"},{"location":"IntelligentSystems.html#history-and-philosophy","text":"Local Search","title":"History and Philosophy"},{"location":"IntelligentSystems.html#local-search","text":"Constraint satisfaction problems","title":"Local Search"},{"location":"IntelligentSystems.html#constraint-satisfaction-problems","text":"In a standard search problem the state is viewed as a black box A state can be any data structure that supports A successor function An objective function (fitness) A goal test In constraint satisfaction problem we will define the data structure of the state In particular, a state is defined by variables x_{i} x_{i} with values from the domain D_{i} D_{i} In the goal test we use a set of constraints Graph colouring problem We try to colour regions with three different colours such that adjacent regions have a different colour. For this map the problem is defined as follows Variables are the regions WA, NT, Q, NSW, V, SA, T Domains are the three colours D_{i} = \\{red, green, blue\\} D_{i} = \\{red, green, blue\\} For example WA and NT cannot have the same colour Complete Every variable is assigned a value Consistent No constraint violated This is an example solution for this problem Every region has a colour with no two adjacent regions sharing a colour This solution is therefore complete and consistent This can be generalised as a constraint graph The nodes are the variables and the edges are the constraint No two connected nodes can have the same assignment In addition, the resulting graph is a binary CSP because each constraint relates two variables (each edge connects two nodes)","title":"Constraint satisfaction problems"},{"location":"IntelligentSystems.html#csps-come-in-different-variants","text":"If the CSP has discrete variables And the domain is finite We have n n variables and the domain size is d d Therefore there exists O(d^{n}) O(d^{n}) complete assignments Examples are boolean CSPs, including boolean satisfiability (SAT) which is NP-Complete And the domain is infinite Examples of infinite domains are integers and strings An example for such a CSP is job scheduling where the variables are the start/end days for each job The constraints could be defined like StartJob_{1} + 5 < StartJob_{3} StartJob_{1} + 5 < StartJob_{3} If the CSP has continuous variables An example is start/end times of the Hubble Space Telescope observations The constraints are linear constraints solvable in polynomial time by linear programming","title":"CSPs come in different variants"},{"location":"IntelligentSystems.html#constraints-come-in-different-variants-too","text":"Unary constraints involve a single variable, for example SA \\neq \\neq green Binary constaints involve pairs of variables, for example SA \\neq \\neq WA Higher-order constraints involve 3 or more variables, for example X_{1} + X_{2} = X_{3} X_{1} + X_{2} = X_{3} Some more real-world CSPs are Assignment problems, e.g who teaches what class Timetabling problems, e.g which class is offered when and where Transportation scheduling Factory scheduling Note how many real-world problems involve real-valued (continuous) variables! Missing developing an algorithm for CSPs Adversarial Games: Minimax","title":"Constraints come in different variants too"},{"location":"IntelligentSystems.html#adversarial-games-minimax","text":"Alpha-beta Pruning","title":"Adversarial Games: Minimax"},{"location":"IntelligentSystems.html#alpha-beta-pruning","text":"Alpha-beta pruning goes even beyond minimax We have strong and weak methods to improve minimax Strong methods take into account the game itself , ie board symmetry in tic tac toe Weak methods can apply to any game , one of which is alpha-beta pruning What is pruning? A good example of pruning is the symmetry in tic tac toe A part of the full game tree looks like this Some of these states are redudant, we can achieve them just by rotating the board By removing the redundant states we can prune the search tree a lot Not every game can be pruned in this way! How does alpha-beta pruning work? Minimax explores the entire tree to a given ply depth It evaluates the leaves It propagates the values of these leaves back up the tree Alpha-beta pruning performs DFS but allows us to prune/disregard certain branches of the tree Alpha represents the lower bound on the node value. It is the worst we can do . It is associated with MAX nodes and since it is the worst it never decreases. Beta represents the upper bound on the node value. It is the best we can do . It is associated with MIN nodes and since it is the best it never increases. If the best we can do on the current branch is less than or equal to the worst we can do elsewhere, there is no point continuing this branch! This is very similar to minimax except we can return early if the alpha/beta values indicate to do so Alpha-beta is guarenteed to give the same values as minimax except we reduce the search space. If the tree is ordered , the time complexity is O(b d/2 ) (minimax is O(b d )!) Perfect ordering is not possible as we wouldn't need alpha-beta in the first place! Planning","title":"Alpha-beta Pruning"},{"location":"IntelligentSystems.html#planning","text":"A plan is a sequence of actions to perform tasks and achieve objectives This means generating and searching over possible plans The classical planning environment is fully oberservable, deterministic, finite, static and discrete. It helps to assist humans in practical applications such as Design and manufacuring Military operations Games Space exploration In the real world we have a huge planning environment. We need need to reduce this environment by using heuristics and decomposing the problem. What is a planning language? In order to define plans we need a planning language A good planning language should Be expressive enough to describe a wide variety of problems Be restrictive enough to allow efficient algorithms to operate on it Be able to take advantage of the logical structure of the problem Examples of planning languages are The STRIPS (Stanford Research Institute Problem Solver) model ADL (Action Description Language) Typically planning languages involve States decompose the world into logical conditions and represent a state as a collection of these conditions, for example OnTable(Box1) \u22c0 OnFloor(Box2) Goals a goal is a partially specified state, if any state contains all the literals (conditions) of the goal then this state is considered a goal state Actions a precondition and an effect, we can only perform the effect if the precondition is already met, for example we can only put a box on a table if it isn't already on the table In an action there needs to exist a substitution for all variables in the precondition! Air cargo transport Part Definition Initial State Init( At(C1, SFO) \u22c0 At(C2, JFK) \u22c0 At(P1, SFO) \u22c0 At(P2, JFK) \u22c0 Cargo(C1) \u22c0 Cargo(C2) \u22c0 Plane(P1) \u22c0 Plane(P2) \u22c0 Airport(JFK) \u22c0 Airport(SFO) ) Goal State Goal( At(C1, JFK) \u22c0 At(C2, SFO) ) Actions Action( Load(c, p, a) PRECOND : At(c, a) \u22c0 At(p, a) \u22c0 Cargo(c) \u22c0 Plane(p) \u22c0 Airport(a) EFFECT : \u00acAt(c, a) \u22c0 In(c, p) ) Example plan [Load(C1, P1, SFO), Fly(P1, SFO, JFK), Load(C2, P2, JFK), Fly(P2, JFK, SFO)] Spare tire problem Part Definition Initial State Init(At(Flat, Axle) \u22c0 At(Spare, trunk)) Goal State Goal(At(Spare, Axle)) Actions Action( Remove(Spare, Trunk) PRECOND : At(Spare, Trunk) EFFECT : \u00acAt(Spare, Trunk) \u22c0 At(Spare, Ground) ) Action( Remove(Flat, Axle) PRECOND : At(Flat, Axle) EFFECT : \u00acAt(Flat, Axle) \u22c0 At(Flat, Ground) ) Action(PutOn(Spare, Axle) PRECOND : At(Spare, Ground) \u22c0 \u00acAt(Flat, Axle) `EFFECT : At(Spare, Axle) \u22c0 \u00acAt(Spare, Ground) ) Action( LeaveOvernight EFFECT : \u00acAt(Spare, Ground) \u22c0 \u00acAt(Spare, Axle) \u22c0 \u00acAt(Spare, trunk) \u22c0 \u00acAt(Flat, Ground) \u22c0 \u00acAt(Flat, Axle) ) Example plan [Remove(Flat, Axle), Remove(Spare, Trunk), PutOn(Spare, Axle)] There are two main approaches to making an algorithm which can come up with a plan Forward search, progression planners do forward state-space search. They consider the effect of all possible actions in a given state and search from the initial state to the goal state . Backward search, regression planners do backward state-space search. They consider what must have been true in the previous state to a goal state and therefore search from the goal state to the initial state . Forward searches are nothing more than any complete graph search algorithm, for example A*. Backward searches find actions which have effects that are currently satisfied and pre-conditions which are preferred. These actions must not undo desired literals . This prevents unnecessary actions being considered and therefore reduces the branching factor. Both progression and regression perform poorly without a good heurisic To find an admissile heuristic there are generally two approaches Use the optimal solution to the relaxed problem, e.g. where we remove all preconditions from actions Use the sub-goal independence assumption: The cost of solving a conjunction of subgoals is approximated by the sum of the costs of solving the sub-problems independently.","title":"Planning"},{"location":"IntelligentSystems.html#partial-order-planning","text":"Progression and regression planning are forms of totally ordered plan searches We can improve this by specifying some actions which can be done in parallel For these actions it doesn't matter which comes first Each plan has 4 components A set of actions (steps of the plan) A set of ordering constraints A < B A < B (A has to come before B) A set of causal links A\\xrightarrow{p}B A\\xrightarrow{p}B or A\\xrightarrow{} p\\xrightarrow{} B A\\xrightarrow{} p\\xrightarrow{} B (A achieves p for B) A set of open preconditions Shoe example Part Definition Initial State Init( ) Goal State Goal( RightShoeOn \u22c0 LeftShoeOn ) Actions Action( RightShoe PRECOND : RightSockOn EFFECT : RightShoeOn ) Action( RightSock PRECOND : EFFECT : RightSockOn ) Action( LeftShoe PRECOND : LeftSockOn EFFECT : LeftShoeOn ) Action( LeftSock PRECOND : EFFECT : LeftSockOn ) Actions = {Rightsock\\\\ Rightshoe\\\\ Leftsock\\\\ Leftshoe\\\\ Start\\\\ Finish} Actions = {Rightsock\\\\ Rightshoe\\\\ Leftsock\\\\ Leftshoe\\\\ Start\\\\ Finish} Orderings = {Rightsock < Rightshoe \\\\ Leftsock < Leftshoe} Orderings = {Rightsock < Rightshoe \\\\ Leftsock < Leftshoe} Links = {Rightsock \\xrightarrow{Rightsockon} Rightshoe \\\\ Leftsock \\xrightarrow{Leftsockon} Leftshoe \\\\ Rightshoe \\xrightarrow{Rightshoeon} Finish \\\\ Leftshoe \\xrightarrow{Leftshowon} Finish} Links = {Rightsock \\xrightarrow{Rightsockon} Rightshoe \\\\ Leftsock \\xrightarrow{Leftsockon} Leftshoe \\\\ Rightshoe \\xrightarrow{Rightshoeon} Finish \\\\ Leftshoe \\xrightarrow{Leftshowon} Finish} We can put either shoe on first it doesn't matter. This lets us create a partially-ordered plan When is a plan final? A consistent plan with no open preconditions is a solution. A plan is consistent if and only if there are no cycles in the ordering constraints and no conflicts with the links. A POP is executed by repeatedly choosing any of the possible next actions. Missing resolving POP","title":"Partial-order Planning"},{"location":"IntelligentSystems.html#biologicallystatistically-inspired-ai","text":"Classification","title":"Biologically/Statistically Inspired AI"},{"location":"IntelligentSystems.html#classification","text":"Reasoning","title":"Classification"},{"location":"IntelligentSystems.html#reasoning","text":"Reasoning deals with updating our \"belief model\" There are different approaches to reasoning Logic/Rule based we build up basic rules (axioms) using logic which we can derive other rules from Stochastic reasoning reasoning based on probabilities Total Probability The total probabilty of an event B is defined as P(B) = \\sum_{a} P(B\\, and\\, (A=a)) = \\sum_{a} P(B|A = a)P(A=a) P(B) = \\sum_{a} P(B\\, and\\, (A=a)) = \\sum_{a} P(B|A = a)P(A=a) The total probability for event B is the sum of the probabilities for event B and all cases of event A Conditional Probability The conditional probability of an event B is defined as P(B|A) = \\frac{P(A\\, and\\, B)}{P(A)} P(B|A) = \\frac{P(A\\, and\\, B)}{P(A)} P(A|B)P(B) = P(A\\, and\\, B) = P(B|A)P(A) P(A|B)P(B) = P(A\\, and\\, B) = P(B|A)P(A) P(A|B) P(A|B) means \"the probability of event A given that B is true\" Why is it defined like this? Let's look at an example to explain this. We have a bag with red and blue marbles, and we want to know the probabilities of drawing a red/blue marble. We can visualise the probabilities for 2 consecutive draws like this What is the probability of drawing a blue marble, and then another blue one? It's a chance of \\frac{2}{5} \\frac{2}{5} followed by a chance of \\frac{1}{4} \\frac{1}{4} so overall it's \\frac{1}{10} \\frac{1}{10} Now define event A to be \"draw a blue marble first\" and define event B to be \"draw a blue marble second\" We have just calculated P(A\\, and\\, B) = P(A) * P(B|A) P(A\\, and\\, B) = P(A) * P(B|A) Bayes' Rule Bayes' rule is defined as P(B|A) = \\frac{P(A|B)P(B)}{P(A)} P(B|A) = \\frac{P(A|B)P(B)}{P(A)} We can derive this rule with the forumlas in conditional probability. Informally this gives us the probability for B given that A is true. We can think of Bayes' rule as P(hypothesis|evidence) = \\frac{P(hypothesis)P(evidence|hypothesis)}{P(evidence)} P(hypothesis|evidence) = \\frac{P(hypothesis)P(evidence|hypothesis)}{P(evidence)} We can caclulate the probability of our hypothesis based on some evidence which are events we have observed already Monty hall problem There are three doors, two hides goats the other hides a car We choose door 1 The host opens door 2 and there is a goat We can now stay with door 1 or swap to door 3 P(win) P(win) if we stay with door 1 is \\frac{1}{3} \\frac{1}{3} as we made our choice when there were 3 doors What if we swap to door 3? P(B) = \\sum_{a} P(B|A = a)P(A=a) P(B) = \\sum_{a} P(B|A = a)P(A=a) (see total probability) Let B = win and A = car behind door 1 (lose) P(win) = P(win | door1) * P(door1) + P(win | \u00acdoor1) * P(\u00acdoor1) P(win) = P(win | door1) * P(door1) + P(win | \u00acdoor1) * P(\u00acdoor1) P(win | door1) = 0 P(win | door1) = 0 we switched to door 3! P(door1) = \\frac{1}{3} P(door1) = \\frac{1}{3} 1 car 2 goats so 1 in 3 P(win | \u00acdoor1) = 1 P(win | \u00acdoor1) = 1 if not behind door1 it must be behind door3 P(\u00acdoor1) = \\frac{2}{3} P(\u00acdoor1) = \\frac{2}{3} as door2 has been opened by the host P(win) = 0 * \\frac{1}{3} + 1 * \\frac{2}{3} = \\frac{2}{3} P(win) = 0 * \\frac{1}{3} + 1 * \\frac{2}{3} = \\frac{2}{3} Independence Two random variables are independent if their joint probability is the product of their probabilities P(A\\, and\\, B) = P(A) * P(B) P(A\\, and\\, B) = P(A) * P(B) Therefore the conditional probabilities of A and B are equal to the probability of the event itself! P(A|B) = \\frac{P(A\\, and\\, B)}{P(B)} = \\frac{P(A) * P(B)}{P(B)} = P(A) P(A|B) = \\frac{P(A\\, and\\, B)}{P(B)} = \\frac{P(A) * P(B)}{P(B)} = P(A) P(B|A) = P(B) P(B|A) = P(B)","title":"Reasoning"},{"location":"IntelligentSystems.html#bayesian-belief-update","text":"In bayesian reasoning we always keep a belief model when we make our decisions We use probabilities to capture uncertainty in our knowledge We also need to update our belief model after we have seen some more evidence How do we update our belief after each observation? P(model|observation) = \\frac{P(model)P(observation|model)}{P(observation)} P(model|observation) = \\frac{P(model)P(observation|model)}{P(observation)} We know all the probabilities on the right side P(model) P(model) is called the prior . It is the probability that the model is true without taking any observation into account. P(observation|model) P(observation|model) is called the likelihood . It is the probability that the observed event occurs given that the model is true. P(observation) P(observation) is the probabilty that the observation occurs in general. We want to calculate the left hand side P(model|observation) P(model|observation) is call the posterior . It is the probability that the model is true given that we have seen the observation.","title":"Bayesian Belief Update"},{"location":"IntelligentSystems.html#complex-knowledge-representation","text":"So far we only considered a simple correlation between probabilities but what if we have a much more complicated network of correlations? How can we apply inference in complex networks (ie how can we apply Baye's rule) there? Joint Distribution The simplest way to do inference is to look at the joint distribution of the probability values Here is an example from an employment survey Male Long hours Rich Probability T T T 0.13 T T F 0.11 T F T 0.10 T F F 0.34 F T T 0.01 F T F 0.04 F F T 0.02 F F F 0.25 Now we can ask questions like \"What is the probability that the person is rich?\" We need to calculate the total probability of P(Rich) P(Rich) As we have the table we can sum up all probabilities where Rich = True Rich = True P(Rich) = 0.13 + 0.10 + 0.01 + 0.02 = 0.26 P(Rich) = 0.13 + 0.10 + 0.01 + 0.02 = 0.26 What about the probability a person works for long hours given that he is a male? We need to calculate P(Longhours | Male) P(Longhours | Male) This is equal to \\frac{P(Longhours\\, and\\, Male)}{P(Male)} \\frac{P(Longhours\\, and\\, Male)}{P(Male)} P(Longhours\\, and\\, Male) = 0.13 + 0.11 = 0.24 P(Longhours\\, and\\, Male) = 0.13 + 0.11 = 0.24 P(Male) = 0.13 + 0.11 + 0.10 + 0.34 = 0.68 P(Male) = 0.13 + 0.11 + 0.10 + 0.34 = 0.68 P(Longhours | Male) = 0.24 / 0.68 = 0.35 P(Longhours | Male) = 0.24 / 0.68 = 0.35 We can do any inference from joint distribution however this does not scale well in practice . For example if we had 30 variables we would need a table with 2 30 entries ! This also does not account for independant variables which we can discard completely! Bayesian Networks We can graphically represent the network like this S = studied for the exam M = lecturer is in a good mood H = high marks Every circle (node) is a random variable Every directed edge from node A to B means B depends on A Therefore H depends on both S and M There is no directed edge between S and M, they are independent What is we wanted to calculate \"Given high marks, what is the probability that the lecturer was in a good mood?\" This is P(M|H) = \\,? P(M|H) = \\,? We will use Bayes' rule P(M|H) = \\frac{P(M)P(H|M)}{P(H)} P(M|H) = \\frac{P(M)P(H|M)}{P(H)} We know that P(M) = 0.3 P(M) = 0.3 but we don't know P(H) P(H) or P(H|M) P(H|M) yet P(H|M)\\, = P(H | M\\, and\\, S) * P(S) + P(H | M\\, and\\, \u00acS) * P(\u00acS)\\\\\\qquad\\qquad = 0.9 * 0.8 + 0.5 * 0.2\\\\\\qquad\\qquad = 0.82 P(H|M)\\, = P(H | M\\, and\\, S) * P(S) + P(H | M\\, and\\, \u00acS) * P(\u00acS)\\\\\\qquad\\qquad = 0.9 * 0.8 + 0.5 * 0.2\\\\\\qquad\\qquad = 0.82 P(H) = P(H| M\\, and\\, S) * P(M\\, and\\, S)\\\\\\qquad\\qquad + P(H | M\\, and\\, \u00acS) * P(M\\, and\\, \u00acS)\\\\\\qquad\\qquad + P(H | \u00acM\\, and\\, S) * P(\u00acM\\, and\\, S)\\\\\\qquad\\qquad + P(H | \u00acM\\, and\\, \u00acS) * P(\u00acM\\, and\\, \u00acS) P(H) = P(H| M\\, and\\, S) * P(M\\, and\\, S)\\\\\\qquad\\qquad + P(H | M\\, and\\, \u00acS) * P(M\\, and\\, \u00acS)\\\\\\qquad\\qquad + P(H | \u00acM\\, and\\, S) * P(\u00acM\\, and\\, S)\\\\\\qquad\\qquad + P(H | \u00acM\\, and\\, \u00acS) * P(\u00acM\\, and\\, \u00acS) As we know M and Sare independant we know their joint probability is just their product P(H) = 0.9 * 0.3 * 0.8 \\\\\\qquad\\qquad + 0.5 * 0.3 * 0.2 \\\\\\qquad\\qquad + 0.4 * 0.7 * 0.8 \\\\\\qquad\\qquad + 0.05 * 0.7 * 0.2 \\\\\\quad\\quad\\;\\; = 0.216 + 0.03 + 0.224 + 0.007 \\\\\\quad\\quad\\;\\; = 0.477 P(H) = 0.9 * 0.3 * 0.8 \\\\\\qquad\\qquad + 0.5 * 0.3 * 0.2 \\\\\\qquad\\qquad + 0.4 * 0.7 * 0.8 \\\\\\qquad\\qquad + 0.05 * 0.7 * 0.2 \\\\\\quad\\quad\\;\\; = 0.216 + 0.03 + 0.224 + 0.007 \\\\\\quad\\quad\\;\\; = 0.477 Now we can finally calculate P(M|H) P(M|H) ! P(M|H) = \\frac{P(M)P(H|M)}{P(H)} = \\frac{0.3 * 0.82}{0.477} = 0.516 P(M|H) = \\frac{P(M)P(H|M)}{P(H)} = \\frac{0.3 * 0.82}{0.477} = 0.516 The probability the lecturer was in a good mood, given high marks, is about 52% Properties of Bayesian Networks Must be directed acyclic graphs Major efficieny is that we have economized on memory Easier for humans to interpret Why is it good to use Bayesian Networks? It captures uncertainty of our knowledge about the environ in a very elegant and simple way We can integrate our prior knowledge into the reasoning by using the prior distribution (which represents our prior knowledge) We can always update our belief about the world by using Baye's Theorem Why is it bad to use Bayesian Networks? If we use a wrong prior then it will be difficult to get correct answers The calculation includes integrals and summing over all possible situations (which is typically computationally very expensive!) Descision Making","title":"Complex Knowledge Representation"},{"location":"IntelligentSystems.html#descision-making","text":"So far we have looked at classification and updating our belief model. Now we will look into actual decision making and updating our decision making policy. We will do this by using 2 simple models. Multi-armed bandits (bandit theory)","title":"Descision Making"},{"location":"IntelligentSystems.html#bandit-theory","text":"Suppose we want to outsouce a robbery. There are three different guys from which we can choose one, but we don't know how effective they are. The expected reward per robbery is 50 for the first guy, -10 for the second guy and 30 for the third guy. But remember we don't know that . The expected reward is an average value, meaning that they won't always achieve exactly this value. Therefore our goal is to find out that the first guy is most effective via repeated robbing in multiple rounds. Each round we choose exactly one guy. Who should we hire each round? It is called exploration when we just try out different guys and record their performance guy 1 guy 2 guy 3 50 -10 30 27 -3 41 69 4 22 55 -17 31 ... ... ... 72 -6 33 ----- average ------ 51.3 -8.6 29.5 We will eventually get to their expected values earlier however there is a limit to the number of rounds! We also want to find the best guy as soon as possible so that we can maximise our total reward! It is called exploitation when we always choose the one which we think is best. There is a trade-off between exploration and exploitation. If we focus too much on exploration We can accurately estimate each guy's expected performance But it will take too much time , and we'll end up with a low total reward If we focus too much on exploitation We can focus on maximising the total reward But we might miss a bandit with a higher reward","title":"Bandit Theory"},{"location":"IntelligentSystems.html#multi-armed-bandit-mab-model","text":"The key challenge is to efficiently balance exploration and exploitation such that we maximise the total reward. There are multiple arms with different distributions (probabilities that we win). We don't know the distribution of the arms. Here is an example model with three arms. We are going to \"play\" with the arms for multiple rounds, where we can select one in each round. Our objective is to maximise the total reward. How explorative or exploitative should we be? The Multi-armed Bandit Model is the simplest model that captures the dilemma of exploration vs exploitation. How can we solve the MAB problem? We dont have knowledge about the expected reward values What is the optimal total value? Can we achieve the optimal total value? Our goal is to design algorithms that are close to the optimum as much as possible (good approximation). There are two different approaches to solving this. Epsilon-first approach Suppose the number of rounds T is fixed Choose an epsilon value 0 < epsilon < 1 (typically between 0.05 and 0.2) In the first epsilon*T rounds we only do exploration by pulling the arms in a round robin manner After that we do exploitation by choosing the arm with the highest average reward value Suppose epsilon is 0.1 and T is 100 In the first 10 rounds (0.1 * 100) we pull all three arms in a round robin manner After the first 10 rounds we calculate the arm with the highest average reward In the last 90 rounds (0.9*100) we pull this arm only Epsilon-greedy appraoch Choose an epsilon value 0 < epsilon < 1 (typically between 0.05 and 0.2) Pull the arm with the current best average reward value with probability 1-epsilon Pull one of the other arms uniformly at random Repeat this for each round Suppose epsilon is 0.1 We pull the arm with the current best average reward 90% of the time We pull a random arm 10% of the time Epsilon-first Epsilon-greedy \u2714\ufe0fTypically very good when T is small \u2714\ufe0fTypically efficient when T is sufficiently large \u2757Needs to know T in advance \u2757Slow convergance at start \u2757Sensitive to epsilon value \u2757Sensitive to epsilon value","title":"Multi-armed Bandit (MAB) Model"},{"location":"IntelligentSystems.html#other-algorithms","text":"There are also other algorithms, not just epsilon-first and epsilon-greedy. One example is Upper Confidence Bound (UCB) which combines explorations and exploitation within each single round in a very clever way. Another example is Thompson-sampling which involves maintaining a belief distribution about the true expect reward of each arm using Bayes' Theorem. Randomly sample from each of these believes then choose the arm with the highest sample. We repeat this each round. Performance of Bandit Algorithms and Regret How can we compare the different algorithms? How can we measure the goodness of an algorithm in general? Can we always achieve the optimal solution (best possible)? Spoilers We cannot achieve the best possible! Our aim is to design algorithms that have close performance to that of the best possible. One measurement to quantify the performance is called regret. Regret is the difference between the performance of an algorithm with that of the best possible for a certain number of steps taken . Let's go back to the robbery example We have 3 arms and we pull 100 times Arm 1, 2 and 3 have the expected rewards 50, -10 and 30 Remember we aren't supposed to know that! The best possible algorithm would choose arm 1 all 100 times, therefore the total reward would be 5000 Suppose our algorithm chooses arm 2 20 times and arm 3 10 times and for the rest it pulls arm 1. The total reward is -10*20 + 30*10 + 50*70 = 3600 The difference (ie regret ) is 5000 - 3600 = 1400 We can then say the regret for this algorithm in 100 time steps is 1400 Does there exist a no-regret algorithm? Hold-on! A no-regret algorithm is an algorithm where its average regret converages to 0 when the number of time steps approaches infinity . This indicates that, on average, the algorithm always pulls the best arm. This also indicates that, on average, a no-regret algorithm will start to behave like the optimal (best) algorithm. That's exactly what we want! Extensions of the Multi-armed Bandit There are some extensions to the multi-armed bandit model. budget-limited bandit we have to pay a cost to pull an arm with a total budget limit dueling bandits we choose 2 bandits at each round and we only check which one is better but not the actual reward values best-arm bandit we aim to identify the best arm, we do this by pure learning (only exploration no exploitation) Markov decision process (reinforcement learning)","title":"Other Algorithms"},{"location":"IntelligentSystems.html#reinforcement-learning","text":"How do humans learn? They learn from experiences . Let's start with an example problem, an agent wants to find the way out from a maze. At each time step the agent Chooses a direction Makes a step Checks whether it's the exit This is a standard search problem! Let's set a new goal for our problem. The agent wants to find the shortest path from A (entrance) to B (exit). What we want to have is a policy of behaviour, at each situation it will tell the agent what to do. Feedback In reinforcement learning, the agent receives feedback on how good the chosen action was There is continuous feedback until the goal is reached It is inbetween supervised and unsupervised learning The agent repeatedly interacts with the environment It gets some feedback, positive or negative , hence reinforcement It is a learning problem because we try to find a good policy based on the feedback A policy is a set of rules that tells the agent what to do at each state How do we know what actions lead us to victory (our goal)? What about those that made us lose the game? How can we measure which action is the best to take at each time step? We need to be able to evaluate the actions we take and the states we are in","title":"Reinforcement Learning"},{"location":"IntelligentSystems.html#states-actions-and-rewards","text":"Let's define states, actions and rewards We can think about the world as a set of states, there are good and bad states (less or more ideal) With an action we move from one state to another The reward is the feedback of the environment, it measures how good an action was We want to maximise the sum of collected rewards over time. Temporal Difference Learning Suppose we try to find the shortest path for some maze. There are six states where one is the terminal state (exit state). We will start at state A and try to reach state F, which gives a reward of 100 All other states, initially, have a reward of 0 We want to maximise the rewards over time, which is equivalent to finding the shortest path At the beginning we don't have any prior knowledge so we have to start with a really simple policy , just randomly move at each state ! Later when they have reward values we will always move to the state with the highest value Sometime later we eventually arrive at F where we receive a reward of 100 What was the last state before F? Surely that state must be pretty good too We then update the value of that state to be good If we repeat this process we are performing temporal difference learning We maintain the value V of each state, they represent how good a state is How are we going to update our esitmate of each state's value? Immediate reward is important, if moving into a certain state gives us a reward or punishment we need to record that straight away! But future reward is important too. A state may give us nothing now but we still like it if it is linked to future reinforcement It is also important not to let any single learning experience change our opinion too much We want to change our V estimates gradually to allow the long-run picture to emerge We need a learning rate The formula for temporal difference learning combines these factors Current reward Future reward Learning rate Suppose we go from state i to state j then we will update the value of state i What is the learning rate? It determines to what extent newly acquired information overrides old information If we choose a=0 then the agent learns nothing If we choose a=1 then the agen only considers the most recent information Ideally we choose some value between 0 and 1 But wait, there is no negative feedback! Eventually all states will reach a max good value! We need some way to distinguish paths that require less moves. To do this we use a discount factor with a value between 0 and 1. Q-Learning In some cases we also need to learn the outcomes of the actions. This is the case if we don't know which actions will take us to which state. In other words we want to learn the quality of taking each action at each state. This is called the Q value for a specific action and a specific state. The Q value for state i and action k which leads to state j* with actions x** is calculated by","title":"States, Actions and Rewards"},{"location":"IntelligentSystems.html#markov-decision-processes-mdp-and-markov-chain","text":"In real world problems the state transitions are often stochastic (random) Here is an example You as a student want to graduate within 4 years Obviously there are actions you can take to achieve this goal However there will be some random chance which plays some role too We can model the probabilities of taking a certain action in a Markov chain Suppose the current state is state 3 If we take the action there is a 50% probability that we go to state 1 , and a 50% probability we go to state 4 If the probability of arriving at the next state only depends on the current state and action (not any previous states) then the process has a Markov property We can also model the probabilities in an n by n matrix P P with P_{ij} P_{ij} being the probability of making a transition from state i to state j After we have define the matrix we can also calculate the probability of being in each state after k steps In our example we take state 1 as start state and model this vector b = (1, 0, 0, 0) b = (1, 0, 0, 0) Now to caculate the probabilities of being in each state after k steps we will simply calculate b^{t}P^{k} b^{t}P^{k} We will get a new column vector with the respective probabilities in each state We can also apply temporal difference and Q-Learning to MDP but we need to add the probability for the state transition","title":"Markov Decision Processes (MDP) and Markov Chain"},{"location":"ProgrammingLanguageConcepts.html","text":"Programming Language Concepts \u21e6 University course These notes were made for the University of Southampton COMP2212 module. Coursework \u21e6 Required features The following is a list of features which must be included for this language to be useful. Accept finite sequences of integers as streams Accept multiple streams Allow general non-terminal stream operations Allow operations which accept (combine) multiple streams Allow global variables (not bound to a stream) Allow recursive operations Restrictions Using the features required above and some insight into the spec we can make the following restrictions. Streams must be fully independent Input Streams must be immutable General design ideas Read in a stream with LoadStreams \"Inputfile.txt\"; That will read in n streams and define the globals InStream0, InStream1, InStream2, ... InStreamN as well as NumInStreams. For output purposes the debug operation Show will print a stream out and pass it on. LoadStreams \"Inputfile.txt\"; InStream0 -> Show; InStream1 -> Show; NumInStreams -> Show; Comments can be added with the use of # # input # # 1 2 # # 5 6 # # 1 5 # LoadStreams \"Inputfile.txt\"; #Load in streams InStream0 -> Show; # 1 5 1 InStream1 -> Show; # 2 6 5 NumInStreams -> Show; # 2 Output streams are also defined in a similar way, OutStream0, OutStream1, OutStream2 ... OutStreamN as well as NumOutStreams. Operations can target these. For example copy which simply accepts an input and passes it on with no modifications. InStream0 -> Copy -> OutStream0; We can combine multiple operations to form more complex behaviours. For example we can show the elements we are copying. # input # # 1 2 # # 5 6 # # 1 5 # LoadStreams \"Inputfile.txt\"; #Load in streams InStream0 -> Show -> Copy -> OutStream0; #Prints 1 5 1 Operations can accept multiple streams to reduce them. An example of this would be an add operation. # input # # 1 2 # # 5 6 # # 1 5 # LoadStreams \"Inputfile.txt\"; #Load in streams InStream0|InStream1 -> Add -> Show -> OutStream0; #Prints 3 11 6 Example solutions for the problems could be made this way Problem1 LoadStreams \"Inputfile.txt\"; Gen 0 -> OutStream0; InStream0 -> Copy -> OutStream0; OutputStreams 1; Problem2 LoadStreams \"Inputfile.txt\"; InStream0 -> Copy -> OutStream0; InStream0 -> Copy -> OutStream1; OutputStreams 2; Problem3 LoadStreams \"Inputfile.txt\"; InStream0 | (InStream1 -> Multiply 3) -> Add -> OutStream0; OutputStreams 1; Problem4 LoadStreams \"Inputfile.txt\" InStream0 -> Previous (Add) -> OutStream0; OutputStreams 1;","title":"Programming Language Concepts"},{"location":"ProgrammingLanguageConcepts.html#programming-language-concepts","text":"University course These notes were made for the University of Southampton COMP2212 module.","title":"Programming Language Concepts"},{"location":"ProgrammingLanguageConcepts.html#coursework","text":"Required features The following is a list of features which must be included for this language to be useful. Accept finite sequences of integers as streams Accept multiple streams Allow general non-terminal stream operations Allow operations which accept (combine) multiple streams Allow global variables (not bound to a stream) Allow recursive operations Restrictions Using the features required above and some insight into the spec we can make the following restrictions. Streams must be fully independent Input Streams must be immutable General design ideas Read in a stream with LoadStreams \"Inputfile.txt\"; That will read in n streams and define the globals InStream0, InStream1, InStream2, ... InStreamN as well as NumInStreams. For output purposes the debug operation Show will print a stream out and pass it on. LoadStreams \"Inputfile.txt\"; InStream0 -> Show; InStream1 -> Show; NumInStreams -> Show; Comments can be added with the use of # # input # # 1 2 # # 5 6 # # 1 5 # LoadStreams \"Inputfile.txt\"; #Load in streams InStream0 -> Show; # 1 5 1 InStream1 -> Show; # 2 6 5 NumInStreams -> Show; # 2 Output streams are also defined in a similar way, OutStream0, OutStream1, OutStream2 ... OutStreamN as well as NumOutStreams. Operations can target these. For example copy which simply accepts an input and passes it on with no modifications. InStream0 -> Copy -> OutStream0; We can combine multiple operations to form more complex behaviours. For example we can show the elements we are copying. # input # # 1 2 # # 5 6 # # 1 5 # LoadStreams \"Inputfile.txt\"; #Load in streams InStream0 -> Show -> Copy -> OutStream0; #Prints 1 5 1 Operations can accept multiple streams to reduce them. An example of this would be an add operation. # input # # 1 2 # # 5 6 # # 1 5 # LoadStreams \"Inputfile.txt\"; #Load in streams InStream0|InStream1 -> Add -> Show -> OutStream0; #Prints 3 11 6 Example solutions for the problems could be made this way Problem1 LoadStreams \"Inputfile.txt\"; Gen 0 -> OutStream0; InStream0 -> Copy -> OutStream0; OutputStreams 1; Problem2 LoadStreams \"Inputfile.txt\"; InStream0 -> Copy -> OutStream0; InStream0 -> Copy -> OutStream1; OutputStreams 2; Problem3 LoadStreams \"Inputfile.txt\"; InStream0 | (InStream1 -> Multiply 3) -> Add -> OutStream0; OutputStreams 1; Problem4 LoadStreams \"Inputfile.txt\" InStream0 -> Previous (Add) -> OutStream0; OutputStreams 1;","title":"Coursework"}]}